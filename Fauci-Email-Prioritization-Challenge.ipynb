{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build an email prioritization model\n",
    "\n",
    "Priority can be defined as how quick an email needed to be dealt with, the content of the email, how quickly it was responded to etc. There's no ground truth on priority, with the hints previously mentioned, you'll need to create your own ground truth (target labels) on priority and explain the rationale behind this. \n",
    "\n",
    "The features you can use is left open, how you model the problem is also left open. We'd say, start with something simple first.\n",
    "\n",
    "Evaluation:  We want to see for a given email that your model has never seen, how good the model is in determining the email's priority. It's recommended that you keep a portion of the dataset here to evaluate your model. Feel free to use standard metrics or create a new ones.\n",
    "\n",
    "Email if you have any questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://docs.python.org/3/library/datetime.html\n",
    "from datetime import datetime\n",
    "def to_timestamp(d):\n",
    "    ''' Given an ISO format date string, convert to unix timestamp '''\n",
    "    return datetime.timestamp(datetime.fromisoformat(d))\n",
    "\n",
    "def to_datetime(d):\n",
    "    ''' Given an ISO format date string, convert to datetime object '''\n",
    "    return datetime.fromisoformat(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nagak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "import re, difflib\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def strip_spaces(s):\n",
    "    return re.sub('[ ]+', ' ', s).strip()\n",
    "\n",
    "def ContentExtraction(mail, ContentSplitters):\n",
    "    '''\n",
    "    Slicing email body content than signature and disclaimers\n",
    "    '''\n",
    "    lmtr = [lmtr for lmtr in ContentSplitters if lmtr in mail.lower()]\n",
    "    if len(lmtr)>0:\n",
    "        limiter = re.search(lmtr[0], mail.lower()).start()\n",
    "        return mail[:limiter]\n",
    "    return mail\n",
    "\n",
    "def Remove_URLs(x):\n",
    "    '''\n",
    "    Removing URLs from the mail body\n",
    "    '''\n",
    "    x = word_tokenize(x)\n",
    "    x = [i for i in x if not len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+',i))]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_mail(text, punctuation=False):\n",
    "    '''\n",
    "    Preprocessing the email content \n",
    "    removing punctuations except few required\n",
    "    strip spaces\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('[^A-Za-z0-9 ,.]+', '', text.replace('\\n',' ')).lower()\n",
    "    text = strip_spaces(text)\n",
    "    text = text.replace(',',' , ').replace('.',' . ')\n",
    "    text = ContentExtraction(text, ContentSplitters)\n",
    "    text = Remove_URLs(text)\n",
    "    if punctuation:\n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text.replace('\\n',' ')).lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "ContentSplitters  = ['best regards', 'rgds ','b rgds', '\\ngreetings', '\\nthanks.', '\\nthanks,', '\\nthank you','\\nthank you,', '\\nthank you\\n', 'sincerely', 'regard ',\n",
    "                      'regards', 'kind regards', 'the information contained in this','forwarded', '\\ntel:', '\\nMobile:', '\\nall the best,','\\ncordially',\n",
    "                      '[image: image.png]','thx','Tel:','Fax:','greeting', '\\nproject manager ', 'from:', 'envoyé :', \n",
    "                      'the information contained in this email are confid', '------- forwarded message -----', \n",
    "                      'proprietary and confidential.','\\nthanks a lot', 'tel. ','Please consider your environmental responsibility',\n",
    "                      'The administrator of your personal data','the information transmitted in this e-mai',  'Deze email en de bijgevoegde', 'this e-mail is intended only for the person or entity']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building a trie to add and remove words faster way\n",
    "class Trie:\n",
    "    head = {}\n",
    "    \n",
    "    def add(self,word):\n",
    "\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                cur[ch] = {}\n",
    "            cur = cur[ch]\n",
    "        cur['*'] = True\n",
    "\n",
    "    def search(self,word):\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                return False\n",
    "            cur = cur[ch]\n",
    "\n",
    "        if '*' in cur:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def printf(self):\n",
    "        print (self.head)\n",
    "\n",
    "# %%timeit\n",
    "# trie_dict.search(\"paper\")\n",
    "# 503 ns ± 1.57 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
    "# %%timeit\n",
    "# 'paper' in words1\n",
    "# 1.26 ms ± 76.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "\n",
    "# Tried with recursive manner of linking mails\n",
    "# df = df.sort_values('time')\n",
    "# df = df[~df[['uniqueID','uniqueID_rsp']].duplicated()]\n",
    "# seq = []\n",
    "# for ind, row in df.iterrows():\n",
    "#     li = [[row['uniqueID'], row['time'].timestamp()], [row['uniqueID_rsp'], row['time_rsp'].timestamp()]]\n",
    "#     seq.append(sorted(li, key=lambda x: x[1]))\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "def get_nearest_word(search_w, stopwords, words1, excess=1000):\n",
    "    '''\n",
    "    Function to search nearest possible word, like \n",
    "    c[icking -> clicking\n",
    "    corrections to mail words mentioned in mail body\n",
    "    \n",
    "    '''\n",
    "\n",
    "#     words1 = [list(map(ord, i)) for i in set(words)]\n",
    "#     max_length = max(map(len, words1))\n",
    "#     words1 = pad_sequences(words1, max_length)\n",
    "#     tree = KDTree(words1, leaf_size=8)\n",
    "    if search_w in stopwords:\n",
    "        return [search_w, 1]\n",
    "    dist = 0\n",
    "    left = 0\n",
    "    right = len(words1)\n",
    "    while left<=right:\n",
    "        mid = (left+right)//2\n",
    "        d = difflib.SequenceMatcher(None, search_w ,words1[mid]).ratio()\n",
    "        if d==1:\n",
    "            break\n",
    "        elif d<dist:\n",
    "            left =  mid + 1\n",
    "        else:\n",
    "            dist = d\n",
    "            right = mid - 1\n",
    "    \n",
    "    dist = ['',0, 0]\n",
    "    for ind, w in enumerate(words1[mid-excess:mid+excess]):\n",
    "        d = difflib.SequenceMatcher(None, search_w, w).ratio()\n",
    "        if d==1:\n",
    "            dist = [w, d]\n",
    "            return dist\n",
    "        if d>dist[1]:\n",
    "            dist = [w, d]\n",
    "    return dist\n",
    "\n",
    "def concatenate_words(s, i, stopwords, words1, excess, word_size=9):\n",
    "    '''\n",
    "    search and concatenating the space delimited words \n",
    "    fell ow  -> fellow\n",
    "    coordin ate  -> coordinate\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    te = []\n",
    "    for j in [0,-1]:\n",
    "        search_w = []\n",
    "        if j==-1 and i>0:\n",
    "            search_w.append(s[i-1])\n",
    "        threshold = 0\n",
    "        for ind, w in enumerate(s[i:]):\n",
    "            \n",
    "            if threshold>word_size: break\n",
    "            threshold+=len(w)\n",
    "            search_w.append(w)\n",
    "#             print(search_w)\n",
    "#             print(''.join(search_w) in words1)\n",
    "#             r = get_nearest_word(''.join(search_w), stopwords, words1, excess=excess)\n",
    "            \n",
    "#             if r[1]==1:\n",
    "#                 return ind+1, r[0], j\n",
    "#             te.append(r)\n",
    "#             if ''.join(search_w) in words1: #r[1]==1:\n",
    "            if trie_dict.search(''.join(search_w)):\n",
    "                return ind+1, ''.join(search_w), j\n",
    "            \n",
    "    return None, '', None\n",
    "\n",
    "\n",
    "\n",
    "def sequence_processing(s, stopwords, words1, excess, word_size=6):\n",
    "    '''\n",
    "    sequence processing from to add space delimited words in sequnce\n",
    "    \n",
    "    sequence :\n",
    "    'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on \n",
    "    international issues of critical significance .'\n",
    "    \n",
    "    Out:\n",
    "    'wion is uniquely positioned as the global of india , presenting its own perspective on international\n",
    "    issues of critical significance .'\n",
    "    \n",
    "    '''\n",
    "    res = []\n",
    "    w = 0\n",
    "    while w<len(s):\n",
    "        # d = get_nearest_word(s[w], stopwords, words1, excess=excess)\n",
    "        if trie_dict.search(s[w]):\n",
    "            res.append(s[w])\n",
    "        else:\n",
    "            d1 = concatenate_words(s, w, stopwords, words1, excess, word_size)\n",
    "            if d1[2]==-1:\n",
    "                res.pop()\n",
    "                res.append(d1[1])\n",
    "            elif d1[2]==0:\n",
    "                res.append(d1[1])\n",
    "                w += d1[0]\n",
    "            else:\n",
    "                res.append(s[w])\n",
    "        w +=1\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "# Building a trie of english available words\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Selecting vocal library of english words \n",
    "words = nltk.corpus.brown.words()\n",
    "words1 = []\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    w = re.sub('[0-9]','', w)\n",
    "    words1.append(w)\n",
    "words1 = list(set(words1))\n",
    "words1.sort()\n",
    "\n",
    "# adding all possible words to trie to make ease of search\n",
    "trie_dict = Trie()\n",
    "for word in words1:\n",
    "    if '*' not in word and word not in list(string.ascii_lowercase+string.ascii_uppercase+string.digits):\n",
    "        trie_dict.add(word)\n",
    "\n",
    "\n",
    "# %%time\n",
    "# s = 'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on international issues of critical significance .'\n",
    "# sequence_processing(s.split(' '), stopwords, words1, tune)\n",
    "# Out: 'wion is uniquely positioned as the global of india , presenting its own perspective on international issues of critical significance .'\n",
    "# before code optimization\n",
    "# Wall time: 8.43 s\n",
    "\n",
    "# after code optimization\n",
    "# 38.2 µs ± 893 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "\n",
    "\n",
    "# Exception need to be handled\n",
    "# s = 'if you are interested and available to talk to us through a phone call at 1030 am friday or saturd ay this w e e k , or sometime betwee n 930 and 1030 am next mond ay , please let us know .'\n",
    "# sequence_processing('this w e e k , or'.split(' '), stopwords, words1, lr, word_size=15)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "email_chain_pairs = pickle.load(open('email_chains.pkl','rb'))\n",
    "\n",
    "\n",
    "ec = email_chain_pairs\n",
    "ml = pd.DataFrame([i[0] for i in ec])\n",
    "rs = pd.DataFrame([i[1] for i in ec])\n",
    "\n",
    "ml['time']= pd.to_datetime(ml['time'], utc=True)\n",
    "rs['time'] = pd.to_datetime(rs['time'], utc=True)\n",
    "df = ml.join(rs.add_suffix('_rsp'))\n",
    "df['target'] = df['time_rsp'] - df['time']\n",
    "\n",
    "df['uniqueID'] = df[['sender', 'time', 'subject']].astype(str).apply(' - '.join,1)\n",
    "df['uniqueID_rsp'] = df[['sender_rsp', 'time_rsp', 'subject_rsp']].astype(str).apply(' - '.join,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['mail'] = np.where((df['time_rsp']-df['time'])<pd.to_timedelta('1s'), df['body_rsp'], df['body'])\n",
    "df['text'] =df['mail'].apply(preprocess_mail)\n",
    "df['target'] = df['time_rsp'].subtract(df['time']).dt.total_seconds().abs()\n",
    "df['text1'] = df.apply(lambda x: ''.join(sequence_processing(sent.split(' '), stopwords, words1, excess=30000, word_size=15) for sent in nltk.tokenize.sent_tokenize(x['text'])),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was a post doctoral fellow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'i was a post doctoral fe llow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there '\n",
    "sequence_processing(s.split(' '), stopwords, words1, excess=30000, word_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function intended map loop chain of mails recieved and forwarded from respective id in recursive manner\n",
    "\n",
    "Analysis purpose - not utilized at here\n",
    "'''\n",
    "# def recur(sq, key, res):\n",
    "#     print(key)\n",
    "#     for s in range(len(sq)):\n",
    "#         print(sq[s][2])\n",
    "#         if sq[s][2]==0 and sq[s][0][0]==key:\n",
    "#             print('got in')\n",
    "#             sq[s][2] = 1\n",
    "#             sq, res = recur(sq, sq[s][1][0], res+[sq[s][0][0]])\n",
    "#     return sq, res\n",
    "        \n",
    "#       results = []\n",
    "# for i in range(len(seq))[:3]:\n",
    "#     if key == seq[i][1][0]:\n",
    "#         print('found')\n",
    "#     if seq[i][2]==0:\n",
    "#         seq[i][2]=1\n",
    "#         seq, res = recur(seq, seq[i][1][0], [seq[i][1][0]])\n",
    "#         results.append(res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # x = ml.sample(1).iloc[0]\n",
    "# x = df.sample(1).iloc[0]\n",
    "\n",
    "# sents = []\n",
    "# lr=4000\n",
    "# for s in nltk.tokenize.sent_tokenize(x['text']):\n",
    "#     sent = sequence_processing(s.split(' '), stopwords, words1, lr, word_size=15)\n",
    "#     sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr . fauci , how are you sir long ago , in the early 90s , i was a post doctoral fe llow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there . now i work on brain injury and alzheimers but my interest in viruses and mechanisms of viral pathogenesis has not waned and the recent covid19 outbreak prompted me to do a little investigation on my own . my studies of blood microrna changes after tbi and ad suggest that principal component analysis of distinct changes in circulating mirnas can identify the patient population . microrna alterations can be measured by realtime pcrwhich i presume is the basis of the test that is developed for this disease but i am analyzing blood mi rnaseq expression profiles and now it is possible to quickly sequence blood samples in a few hours and get accurate results . blood gene expression in my studies was more var iable lots of rnasesin blood so i found that micrornas are much more stable in blood and serum samples . i attach an example of a pcahierarchical clustering heatmap analysis of a geo dataset for merscov from 2016 https www . ncbi . n lm . nih . govgeoqueryacc . cgiaccgse8l852 i performed the pca and heatmap analyses at two differen t stringencies and you can see that the patients can be unequivocably distinguished from the controls at very significant p and fdrvalues . just a thought but many clinical centers , hospitals , academic institutions can quickly perfo rm transcriptomewide sequencing . blood rna can be isolated in 12 hrs , sequencing libraries made in a few hres and one mi rna sequencing run can handle up to 48 samples and the data can be quickly analyzed . just my two cents on how nih could accelerate the analysis of new blood samples for this new strain of coronanvirus . you could mobilize hundreds of sequencing centers to help in the analysis .\n"
     ]
    }
   ],
   "source": [
    "x = df.sample(1).iloc[0]\n",
    "print(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.fauci , how are you sir long ago , in the early 90s , i was a post doctoral fellow in the laboratory of viral and molecular pathogenesis at nih .i dont know if the same lab is still there .now i work on brain injury and alzheimers but my interest in viruses and mechanisms of viral pathogenesis has not waned and the recent covid19 outbreak prompted me to do a little investigation on my own .my studies of blood microrna changes after tbi and ad suggest that principal component analysis of distinct changes in circulating mirnas can identify the patient population .microrna alterations can be measured by realtime pcrwhich i presume is the basis of the test that is developed for this disease but i am analyzing blood mi rnaseq expression profiles and now it is possible to quickly sequence blood samples in a few hours and get accurate results .blood gene expression in my studies was more variable of rnasesin blood so i found that micrornas are much more stable in blood and serum samples .i attach an example of a pcahierarchical clustering heatmap analysis of a geo dataset for merscov from 2016 https www .ncbi .n lm .nih .govgeoqueryacc .cgiaccgse8l852 i performed the pca and heatmap analyses at two different and you can see that the patients can be unequivocably distinguished from the controls at very significant p and fdrvalues .just a thought but many clinical centers , hospitals , academic institutions can quickly perform sequencing .blood rna can be isolated in 12 hrs , sequencing libraries made in a few hres and one mi rna sequencing run can handle up to 48 samples and the data can be quickly analyzed .just my two cents on how nih could accelerate the analysis of new blood samples for this new strain of coronanvirus .you could mobilize hundreds of sequencing centers to help in the analysis .\n"
     ]
    }
   ],
   "source": [
    "print(x['text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "def preprocesse_text(df, test_size=0.2, stopwords=[]):\n",
    "    '''\n",
    "    Building a N-gram based model with words sequence from 1 to 6 words \n",
    "    restring max features to 45000\n",
    "    added character based features \n",
    "    \n",
    "    concatenated both feature vectors in modelling\n",
    "    \n",
    "    '''\n",
    "    train_text,test_text,y_train,y_test = train_test_split(df['text1'], df['target'], test_size=test_size)\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(1, 6),\n",
    "        max_features=45000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(5, 6),\n",
    "        max_features=10000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    return train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plotting the model predictions with percent of change from the actual\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer = preprocesse_text(df, test_size=0.2, stopwords=nltk.corpus.stopwords.words('english'))\n",
    "reg = LinearRegression()\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     55000.000000\n",
       "mean      17070.625363\n",
       "std       34140.034756\n",
       "min           0.000000\n",
       "25%        2134.910436\n",
       "50%        4930.414706\n",
       "75%       17130.933567\n",
       "max      973998.906526\n",
       "Name: wgt, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Feature weights analyzation to either keep or discard from modelling\n",
    "'''\n",
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()\n",
    "features['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would possible',\n",
       " 'moving forward gratifying speed commitment',\n",
       " 'covid19 effortsin china zy',\n",
       " 'hit chris',\n",
       " ' sick',\n",
       " 'health care system novel coronavirus outbreak',\n",
       " 'epidemic administered exposed people',\n",
       " 'spokewith',\n",
       " 'confir',\n",
       " 'nonspecific immune',\n",
       " 'system staff working',\n",
       " 'tedros tweet stephanie nebehay stephanie nebehay',\n",
       " 'undetected many unknowns sarscov2',\n",
       " 'work help',\n",
       " 'test result permitted board flights']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chekcking model weights which are  most prominant in modelling\n",
    "thr = features['wgt'].quantile(0.5)\n",
    "features[features['wgt'].gt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['international english news',\n",
       " 'cobtoday hi patty reporter writing ncaas',\n",
       " 'cell symposium emerging reemerging viruses 2017',\n",
       " 'workforce expansion e g widescale',\n",
       " 'workforce expansion e g',\n",
       " 'allin',\n",
       " '1 welcome introductions 2 background proposal',\n",
       " 'could syndicate parts externally social channels',\n",
       " 'advance team international experts led world',\n",
       " 'keen hear man',\n",
       " 'reviewed edited attached response appreciateyour expedited',\n",
       " 'floor new york',\n",
       " 'clinics dont typically undergo fit',\n",
       " 'commend extraordinary efforts government',\n",
       " '13th dec']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chekcking model weights which are less contributing to model \n",
    "features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['gram_type'] = ['ngrams']*train_char_features.shape[1] + ['char grams']*train_word_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gram_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>char grams</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>12442.107330</td>\n",
       "      <td>30333.256634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1507.970038</td>\n",
       "      <td>3784.999534</td>\n",
       "      <td>9544.394225</td>\n",
       "      <td>973998.906526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>37898.956512</td>\n",
       "      <td>41712.438860</td>\n",
       "      <td>1.972068</td>\n",
       "      <td>10559.129773</td>\n",
       "      <td>25344.885495</td>\n",
       "      <td>48930.253585</td>\n",
       "      <td>355834.796445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean           std       min           25%  \\\n",
       "gram_type                                                                 \n",
       "char grams  45000.0  12442.107330  30333.256634  0.000000   1507.970038   \n",
       "ngrams      10000.0  37898.956512  41712.438860  1.972068  10559.129773   \n",
       "\n",
       "                     50%           75%            max  \n",
       "gram_type                                              \n",
       "char grams   3784.999534   9544.394225  973998.906526  \n",
       "ngrams      25344.885495  48930.253585  355834.796445  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking the model weights in consideration to character and n gram based feature words\n",
    "'''\n",
    "features.groupby('gram_type')['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1504.0</td>\n",
       "      <td>37470.965327</td>\n",
       "      <td>42563.968379</td>\n",
       "      <td>70.839886</td>\n",
       "      <td>9640.877179</td>\n",
       "      <td>24549.970594</td>\n",
       "      <td>49157.641113</td>\n",
       "      <td>328389.018847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>39812.695132</td>\n",
       "      <td>43529.448213</td>\n",
       "      <td>102.503251</td>\n",
       "      <td>11232.179002</td>\n",
       "      <td>26086.826227</td>\n",
       "      <td>50986.680053</td>\n",
       "      <td>353352.973463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1714.0</td>\n",
       "      <td>36714.078229</td>\n",
       "      <td>39835.147428</td>\n",
       "      <td>41.745847</td>\n",
       "      <td>10253.830283</td>\n",
       "      <td>24792.048434</td>\n",
       "      <td>48096.406276</td>\n",
       "      <td>308471.618034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1634.0</td>\n",
       "      <td>37075.183352</td>\n",
       "      <td>40680.707996</td>\n",
       "      <td>1.972068</td>\n",
       "      <td>10683.856881</td>\n",
       "      <td>25336.759384</td>\n",
       "      <td>47659.325324</td>\n",
       "      <td>343383.063939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1581.0</td>\n",
       "      <td>37373.377809</td>\n",
       "      <td>40046.638510</td>\n",
       "      <td>186.542120</td>\n",
       "      <td>10594.329381</td>\n",
       "      <td>25047.103121</td>\n",
       "      <td>48113.730421</td>\n",
       "      <td>355834.796445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1551.0</td>\n",
       "      <td>38539.490574</td>\n",
       "      <td>43177.281371</td>\n",
       "      <td>4.938738</td>\n",
       "      <td>10103.697955</td>\n",
       "      <td>25531.056259</td>\n",
       "      <td>49970.715875</td>\n",
       "      <td>342254.488260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std         min           25%  \\\n",
       "features                                                                 \n",
       "1         1504.0  37470.965327  42563.968379   70.839886   9640.877179   \n",
       "2         2016.0  39812.695132  43529.448213  102.503251  11232.179002   \n",
       "3         1714.0  36714.078229  39835.147428   41.745847  10253.830283   \n",
       "4         1634.0  37075.183352  40680.707996    1.972068  10683.856881   \n",
       "5         1581.0  37373.377809  40046.638510  186.542120  10594.329381   \n",
       "6         1551.0  38539.490574  43177.281371    4.938738  10103.697955   \n",
       "\n",
       "                   50%           75%            max  \n",
       "features                                             \n",
       "1         24549.970594  49157.641113  328389.018847  \n",
       "2         26086.826227  50986.680053  353352.973463  \n",
       "3         24792.048434  48096.406276  308471.618034  \n",
       "4         25336.759384  47659.325324  343383.063939  \n",
       "5         25047.103121  48113.730421  355834.796445  \n",
       "6         25531.056259  49970.715875  342254.488260  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking the most prominant n-gram size for modelling\n",
    "'''\n",
    "features[:train_char_features.shape[1]].groupby(features['features'].str.split(' ').apply(len))['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Considering the analysis of model weights and removing unsupportive key words which are not contributing to model\n",
    "'''\n",
    "extra_stopwords = features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Re initiating the model by removing the inappropriate key words as stopwords and ngram selection\n",
    "\n",
    "'''\n",
    "\n",
    "train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer = preprocesse_text(df, test_size=0.2, stopwords=nltk.corpus.stopwords.words('english')+extra_stopwords)\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_score = np.mean(cross_val_score(reg, train_features, y_train.values, cv=3, scoring='r2'))\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the generated model\n",
    "with open('regressor.pkl','wb') as f:\n",
    "    pickle.dump(reg, f)\n",
    "    pickle.dump(word_vectorizer, f)\n",
    "    pickle.dump(char_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
