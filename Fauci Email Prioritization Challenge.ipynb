{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build an email prioritization model\n",
    "\n",
    "Priority can be defined as how quick an email needed to be dealt with, the content of the email, how quickly it was responded to etc. There's no ground truth on priority, with the hints previously mentioned, you'll need to create your own ground truth (target labels) on priority and explain the rationale behind this. \n",
    "\n",
    "The features you can use is left open, how you model the problem is also left open. We'd say, start with something simple first.\n",
    "\n",
    "Evaluation:  We want to see for a given email that your model has never seen, how good the model is in determining the email's priority. It's recommended that you keep a portion of the dataset here to evaluate your model. Feel free to use standard metrics or create a new ones.\n",
    "\n",
    "Email if you have any questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://docs.python.org/3/library/datetime.html\n",
    "from datetime import datetime\n",
    "def to_timestamp(d):\n",
    "    ''' Given an ISO format date string, convert to unix timestamp '''\n",
    "    return datetime.timestamp(datetime.fromisoformat(d))\n",
    "\n",
    "def to_datetime(d):\n",
    "    ''' Given an ISO format date string, convert to datetime object '''\n",
    "    return datetime.fromisoformat(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "import re, difflib\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def strip_spaces(s):\n",
    "    return re.sub('[ ]+', ' ', s).strip()\n",
    "\n",
    "def ContentExtraction(mail, ContentSplitters):\n",
    "    lmtr = [lmtr for lmtr in ContentSplitters if lmtr in mail.lower()]\n",
    "    if len(lmtr)>0:\n",
    "        limiter = re.search(lmtr[0], mail.lower()).start()\n",
    "        return mail[:limiter]\n",
    "    return mail\n",
    "\n",
    "def Remove_URLs(x):\n",
    "    x = word_tokenize(x)\n",
    "    x = [i for i in x if not len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+',i))]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_mail(text, punctuation=False):\n",
    "    text = re.sub('[^A-Za-z0-9 ,.]+', '', text.replace('\\n',' ')).lower()\n",
    "    text = strip_spaces(text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = text.replace(',',' , ').replace('.',' . ')\n",
    "    text = ContentExtraction(text, ContentSplitters)\n",
    "    text = Remove_URLs(text)\n",
    "    if punctuations:\n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text.replace('\\n',' ')).lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "ContentSplitters  = ['best regards', 'rgds ','b rgds', '\\ngreetings', '\\nthanks.', '\\nthanks,', '\\nthank you','\\nthank you,', '\\nthank you\\n', 'sincerely', 'regard ',\n",
    "                      'regards', 'kind regards', 'the information contained in this','forwarded', '\\ntel:', '\\nMobile:', '\\nall the best,','\\ncordially',\n",
    "                      '[image: image.png]','thx','Tel:','Fax:','greeting', '\\nproject manager ', 'from:', 'envoyé :', \n",
    "                      'the information contained in this email are confid', '------- forwarded message -----', \n",
    "                      'proprietary and confidential.','\\nthanks a lot', 'tel. ','Please consider your environmental responsibility',\n",
    "                      'The administrator of your personal data','the information transmitted in this e-mai',  'Deze email en de bijgevoegde', 'this e-mail is intended only for the person or entity']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Trie:\n",
    "    head = {}\n",
    "\n",
    "    def add(self,word):\n",
    "\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                cur[ch] = {}\n",
    "            cur = cur[ch]\n",
    "        cur['*'] = True\n",
    "\n",
    "    def search(self,word):\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                return False\n",
    "            cur = cur[ch]\n",
    "\n",
    "        if '*' in cur:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def printf(self):\n",
    "        print (self.head)\n",
    "\n",
    "# %%timeit\n",
    "# trie_dict.search(\"paper\")\n",
    "# 503 ns ± 1.57 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
    "# %%timeit\n",
    "# 'paper' in words1\n",
    "# 1.26 ms ± 76.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "\n",
    "def get_nearest_word(search_w, stopwords, words1, excess=1000):\n",
    "\n",
    "#     words1 = [list(map(ord, i)) for i in set(words)]\n",
    "#     max_length = max(map(len, words1))\n",
    "#     words1 = pad_sequences(words1, max_length)\n",
    "#     tree = KDTree(words1, leaf_size=8)\n",
    "    if search_w in stopwords:\n",
    "        return [search_w, 1]\n",
    "    dist = 0\n",
    "    left = 0\n",
    "    right = len(words1)\n",
    "    while left<=right:\n",
    "        mid = (left+right)//2\n",
    "        d = difflib.SequenceMatcher(None, search_w ,words1[mid]).ratio()\n",
    "        if d==1:\n",
    "            break\n",
    "        elif d<dist:\n",
    "            left =  mid + 1\n",
    "        else:\n",
    "            dist = d\n",
    "            right = mid - 1\n",
    "    \n",
    "    dist = ['',0, 0]\n",
    "    for ind, w in enumerate(words1[mid-excess:mid+excess]):\n",
    "        d = difflib.SequenceMatcher(None, search_w, w).ratio()\n",
    "        if d==1:\n",
    "            dist = [w, d]\n",
    "            return dist\n",
    "        if d>dist[1]:\n",
    "            dist = [w, d]\n",
    "    return dist\n",
    "\n",
    "def concatenate_words(s, i, stopwords, words1, excess, word_size=9):\n",
    "    te = []\n",
    "    for j in [0,-1]:\n",
    "        search_w = []\n",
    "        if j==-1 and i>0:\n",
    "            search_w.append(s[i-1])\n",
    "        threshold = 0\n",
    "        for ind, w in enumerate(s[i:]):\n",
    "            \n",
    "            if threshold>word_size: break\n",
    "            threshold+=len(w)\n",
    "            search_w.append(w)\n",
    "#             print(search_w)\n",
    "#             print(''.join(search_w) in words1)\n",
    "#             r = get_nearest_word(''.join(search_w), stopwords, words1, excess=excess)\n",
    "            \n",
    "#             if r[1]==1:\n",
    "#                 return ind+1, r[0], j\n",
    "#             te.append(r)\n",
    "#             if ''.join(search_w) in words1: #r[1]==1:\n",
    "            if trie_dict.search(''.join(search_w)):\n",
    "                return ind+1, ''.join(search_w), j\n",
    "            \n",
    "    return None, '', None\n",
    "\n",
    "\n",
    "\n",
    "def sequence_processing(s, stopwords, words1, excess, word_size=6):\n",
    "    res = []\n",
    "    w = 0\n",
    "    while w<len(s):\n",
    "        # d = get_nearest_word(s[w], stopwords, words1, excess=excess)\n",
    "        if trie_dict.search(s[w]):\n",
    "            res.append(s[w])\n",
    "        else:\n",
    "            d1 = concatenate_words(s, w, stopwords, words1, excess, word_size)\n",
    "            if d1[2]==-1:\n",
    "                res.pop()\n",
    "                res.append(d1[1])\n",
    "            elif d1[2]==0:\n",
    "                res.append(d1[1])\n",
    "                w += d1[0]\n",
    "            else:\n",
    "                res.append(s[w])\n",
    "        w +=1\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "# %%time\n",
    "# s = 'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on international issues of critical significance .'\n",
    "# sequence_processing(s.split(' '), stopwords, words1, tune)\n",
    "# before code optimization\n",
    "# Wall time: 8.43 s\n",
    "\n",
    "# after code optimization\n",
    "# 38.2 µs ± 893 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trie_dict = Trie()\n",
    "for word in words1:\n",
    "    if '*' not in word:\n",
    "        trie_dict.add(word)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ml['text'] = ml['body'].apply(preprocess_mail)\n",
    "# x = ml.sample(1).iloc[0]\n",
    "# s = nltk.tokenize.sent_tokenize(x['text'])[0]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mail'] = np.where((df['time_rsp']-df['time'])<pd.to_timedelta('1s'), df['body_rsp'], df['body'])\n",
    "df['text'] =df['mail'].apply(preprocess_mail)\n",
    "df['target'] = df['time_rsp'].subtract(df['time']).dt.total_seconds().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ml.sample(1).iloc[0]\n",
    "sents = []\n",
    "lr=4000\n",
    "for s in nltk.tokenize.sent_tokenize(x['text']):\n",
    "    sent = sequence_processing(s.split(' '), stopwords, words1, lr)\n",
    "    sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------             (b) (6)   >\n",
      "Hi,\n",
      "NIAD doesn't have ongoing trials with chloroquine and I heard that such tr ials are being conducted in\n",
      "China.\n",
      "We are planning to evaluate chloroquine in animal models ASAP. If any the rapeutic with a good clinical\n",
      "safety profile ( like chloroquine) appears to be effective in vivo against COVID-19, we are planning to\n",
      "quickly add to them to the Remdesivi r therapeutic trial as another arm.\n",
      "Let me know if you have other questions .\n",
      "Kind regards,\n",
      "Cristina\n",
      "Cristina Cassetti, Ph.D.\n",
      "Deputy Director\n",
      "Division of Microbiology and Infectious Diseases\n",
      "National Institute of Allergy and Infectious Diseases, NIH\n",
      "560 1 Fishers Lane, Room 7GS1\n",
      "Rockville , MD 20852\n",
      "Tel:             (b)(6)\n",
      "(b)(6)\n"
     ]
    }
   ],
   "source": [
    "x = ml.sample(1).iloc[0]\n",
    "print(x['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'importance high dr  fauci of course  i appreciate how busy you are  so just reaching out again on behalf of the american college of cardiology as noted below  we were hopeful we could steal just an hour of your t ime in june for an important discussion on covid19 and exercise training  this is part of a virtual agenda for one of our national conferences care of the athletic heart with outreach around the world given this is a part of the acc educational platform  we hope th is is something you have the bandwidth for  and simply apprec iate your consideration  i hope to hear from you and thank you for all your service once again  my best  jonathan kim jonathan h  kim md  msc  facc chief of sports cardiology department of medicine  division of cardiology department of orthopedics emory university adju nct assistant professor division of applied physiology georgia inst itute of technology team cardiologist for emory athlet ics  georgia tech athletics  atlanta falcons  atlanta hawks  atlanta braves  and atlanta dream office phone cb6 research profile http medicine  emorv  educardiologyemoryfirst faculty djrectoryprofi le  htmifjkim10 email cb6'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.2 µs ± 893 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = 'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on international issues of critical significance .'\n",
    "sequence_processing(s.split(' '), stopwords, words1, tune, word_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "email_chain_pairs = pickle.load(open('email_chains.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_chain_pairs: List of tuples with (recieved email, action from Fauci - reply/fwd etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "words = nltk.corpus.brown.words()\n",
    "words1 = []\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    w = re.sub('[0-9]','', w)\n",
    "    words1.append(w)\n",
    "words1 = list(set(words1))\n",
    "words1.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = email_chain_pairs\n",
    "ml = pd.DataFrame([i[0] for i in ec])\n",
    "rs = pd.DataFrame([i[1] for i in ec])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml['time']= pd.to_datetime(ml['time'], utc=True)\n",
    "rs['time'] = pd.to_datetime(rs['time'], utc=True)\n",
    "df = ml.join(rs.add_suffix('_rsp'))\n",
    "df['target'] = df['time_rsp'] - df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 345\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['uniqueID'] = df[['sender', 'time', 'subject']].astype(str).apply(' - '.join,1)\n",
    "df['uniqueID_rsp'] = df[['sender_rsp', 'time_rsp', 'subject_rsp']].astype(str).apply(' - '.join,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('time')\n",
    "df = df[~df[['uniqueID','uniqueID_rsp']].duplicated()]\n",
    "seq = []\n",
    "for ind, row in df.iterrows():\n",
    "    li = [[row['uniqueID'], row['time'].timestamp()], [row['uniqueID_rsp'], row['time_rsp'].timestamp()]]\n",
    "    seq.append(sorted(li, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['314 - 2019-12-19 04:24:00+00:00 - ', 1576729440.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recur(sq, key, res):\n",
    "    print(key)\n",
    "    for s in range(len(sq)):\n",
    "        print(sq[s][2])\n",
    "        if sq[s][2]==0 and sq[s][0][0]==key:\n",
    "            print('got in')\n",
    "            sq[s][2] = 1\n",
    "            sq, res = recur(sq, sq[s][1][0], res+[sq[s][0][0]])\n",
    "    return sq, res\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(len(seq))[:3]:\n",
    "    if key == seq[i][1][0]:\n",
    "        print('found')\n",
    "#     if seq[i][2]==0:\n",
    "#         seq[i][2]=1\n",
    "#         seq, res = recur(seq, seq[i][1][0], [seq[i][1][0]])\n",
    "#         results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.concat([ml, rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def Remove_URLs(x):\n",
    "#     x = word_tokenize(x)\n",
    "#     x = [i for i in x if not len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+',i))]\n",
    "#     x = ' '.join(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# def ContentExtraction(mail, ContentSplitters):\n",
    "#     lmtr = [lmtr for lmtr in ContentSplitters if lmtr in mail.lower()]\n",
    "#     if len(lmtr)>0:\n",
    "#         limiter = re.search(lmtr[0], mail.lower()).start()\n",
    "#         return mail[:limiter]\n",
    "#     return mail\n",
    "\n",
    "# def PreprocessMails(mails, content=False):\n",
    "#     ProcessedMails = []\n",
    "#     for mail in mails:\n",
    "#         import re\n",
    "#         text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', mail, flags=re.MULTILINE)\n",
    "#         text = ContentExtraction(text, ContentSplitters)\n",
    "#         text = Remove_URLs(text)\n",
    "#         if content:\n",
    "#             ProcessedMails.append(text)\n",
    "#             continue\n",
    "#         text = tokenize(text)\n",
    "#         text = Lemmatization(text)\n",
    "#         ProcessedMails.append(' '.join(text))\n",
    "#     return ProcessedMails\n",
    "\n",
    "# def FilterWords(text, tokens=None, Nouns=False):\n",
    "#     Verbs = ['VB','VBP','RB','VBN']\n",
    "#     text_tokens = nltk.pos_tag(nltk.tokenize.word_tokenize(text))\n",
    "#     if isinstance(tokens,str) and tokens=='nouns':\n",
    "#         verbs = [i[0] for i in text_tokens if i[1].startswith('NN') or i[1]=='CD']\n",
    "#     elif isinstance(tokens,str) and tokens=='verbs':\n",
    "#         verbs = [i[0] for i in text_tokens if i[1] in Verbs]\n",
    "#     else:\n",
    "#         verbs = [i[0] for i in text_tokens if i[1] in tokens]\n",
    "#     return ' '.join(verbs)\n",
    "\n",
    "# def POS_Extract(mails, parts='verbs', content=False):    \n",
    "#     ProcessedMails = PreprocessMails(mails, content=content)\n",
    "#     text = ' '.join(ProcessedMails)\n",
    "#     if content:\n",
    "#         return text\n",
    "#     text = FilterWords(text, tokens=parts)\n",
    "#     return text\n",
    "\n",
    "# def ContentExtraction(mail, ContentSplitters):\n",
    "#     lmtr = [lmtr for lmtr in ContentSplitters if lmtr in mail.lower()]\n",
    "#     if len(lmtr)>0:\n",
    "#         limiter = re.search(lmtr[0], mail.lower()).start()\n",
    "#         return mail[:limiter]\n",
    "#     return mail\n",
    "\n",
    "# # Prioritize the Delimiter with Thanks and Regards like\n",
    "# MailSplitters = 'Subject:|Envoyé :'\n",
    "# ContentSplitters  = ['best regards', 'rgds ','b rgds', '\\ngreetings', '\\nthanks.', '\\nthanks,', '\\nthank you','\\nthank you,', '\\nthank you\\n', 'sincerely', 'regard ',\n",
    "#                       'regards', 'kind regards', 'forwarded', '\\ntel:', '\\nMobile:', '\\nall the best,','\\ncordially',\n",
    "#                       '[image: image.png]','thx','Tel:','Fax:','greeting', '\\nproject manager ', 'from:', 'envoyé :', \n",
    "#                       'the information contained in this email are confid', '------- forwarded message -----', \n",
    "#                       'proprietary and confidential.','\\nthanks a lot', 'tel. ','Please consider your environmental responsibility',\n",
    "#                       'The administrator of your personal data','the information transmitted in this e-mai',  'Deze email en de bijgevoegde', 'this e-mail is intended only for the person or entity']\n",
    "\n",
    "# Delimiters = '|'.join(['Lastone','regards','Sincerely', 'Thank','Forwarded', 'The Information', \n",
    "#                        'tel-mob-email-mobile', 'greeting', 'please note', 'thx', 'Tel:','Fax:'])\n",
    "# # # df = pd.read_excel(r'D:\\Notsaved\\viswanadhapa\\Downloads\\Ticket data.xlsx')\n",
    "# # df2 = df1.loc[df1['T_Description'].ne('None')]\n",
    "# # df2['DescriptionMails'] = df2['T_Description'].fillna('').apply(PreprocessSplitMails)  #.str.split('Subject:|Envoyé :')#.apply(lambda text: [re.split(Delimiters, te)[0] for te in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessMails(mails, content=False):\n",
    "    ProcessedMails = []\n",
    "    for mail in mails:\n",
    "        import re\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', mail, flags=re.MULTILINE)\n",
    "        text = ContentExtraction(text, ContentSplitters)\n",
    "        text = Remove_URLs(text)\n",
    "        if content:\n",
    "            ProcessedMails.append(text)\n",
    "            continue\n",
    "        text = tokenize(text)\n",
    "        text = Lemmatization(text)\n",
    "        ProcessedMails.append(' '.join(text))\n",
    "    return ProcessedMails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', mail, flags=re.MULTILINE)\n",
    "text = ContentExtraction(text, ContentSplitters)\n",
    "text = Remove_URLs(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\nagak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nagak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nagak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Match(a=0, b=0, size=8), Match(a=8, b=8, size=0)]\n",
      "Similarity Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "string1 = \"clicking\"\n",
    "string2 = \"clicking\"\n",
    "\n",
    "temp = difflib.SequenceMatcher(None,string1 ,string2)\n",
    "\n",
    "print(temp.get_matching_blocks())\n",
    "print('Similarity Score: ',temp.ratio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "def preprocesse_text(df, test_size=0.2, stopwords=[]):\n",
    "    train_text,test_text,y_train,y_test = train_test_split(df['text'], df['target'], test_size=test_size)\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(1, 6),\n",
    "        max_features=35000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(5, 6),\n",
    "        max_features=10000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    return train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer = preprocesse_text(df, test_size=0.2, stopwords=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_score = np.mean(cross_val_score(reg, train_features, y_train.values, cv=3, scoring='r2'))\n",
    "reg.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "cv_score = np.mean(cross_val_score(reg, train_features, y_train.values, cv=3, scoring='r2'))\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     45000.000000\n",
       "mean      23458.074251\n",
       "std       40298.754863\n",
       "min           0.000000\n",
       "25%        2550.395038\n",
       "50%        8328.594305\n",
       "75%       28152.683798\n",
       "max      770848.134727\n",
       "Name: wgt, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40298.75486335292"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['wgt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = features['wgt'].quantile(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icate',\n",
       " 'dear mr varadara jan',\n",
       " 'expedite execute whole hhs response',\n",
       " 'human microbiome profe ssor medicine',\n",
       " 'help sara',\n",
       " 'b6 3014964409',\n",
       " 'note dr',\n",
       " 'urldefense proofpoint',\n",
       " 'elected chair 2 years ministers',\n",
       " ' trea',\n",
       " 'ed icu patients covid19 must',\n",
       " 'one countrys',\n",
       " 'b6 b',\n",
       " 'would able take',\n",
       " 'june important discussion covid19']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_p = 2\n",
    "features[features['wgt'].gt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developing infected covid19',\n",
       " 'group idea future',\n",
       " 'tered',\n",
       " ' devi',\n",
       " 'even phase',\n",
       " 'ent c',\n",
       " 'leaders',\n",
       " ' a few',\n",
       " 'g as ',\n",
       " 'wnch111e session oncov 19 tue 310',\n",
       " 'wnch111e session oncov 19 tue',\n",
       " 'family',\n",
       " 'wnch111e session oncov 19',\n",
       " 'n for',\n",
       " 'n for ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['gram_type'] = ['ngrams']*train_char_features.shape[1] + ['char grams']*train_word_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gram_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>char grams</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>17148.913012</td>\n",
       "      <td>35917.197102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2206.358632</td>\n",
       "      <td>5469.385079</td>\n",
       "      <td>15448.952101</td>\n",
       "      <td>770848.134727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>45540.138586</td>\n",
       "      <td>46540.757077</td>\n",
       "      <td>3.217177</td>\n",
       "      <td>14884.715923</td>\n",
       "      <td>33085.178872</td>\n",
       "      <td>58589.767678</td>\n",
       "      <td>601834.998813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean           std       min           25%  \\\n",
       "gram_type                                                                 \n",
       "char grams  35000.0  17148.913012  35917.197102  0.000000   2206.358632   \n",
       "ngrams      10000.0  45540.138586  46540.757077  3.217177  14884.715923   \n",
       "\n",
       "                     50%           75%            max  \n",
       "gram_type                                              \n",
       "char grams   5469.385079  15448.952101  770848.134727  \n",
       "ngrams      33085.178872  58589.767678  601834.998813  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.groupby('gram_type')['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1852.0</td>\n",
       "      <td>45070.243919</td>\n",
       "      <td>44759.903113</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>15074.730448</td>\n",
       "      <td>33409.543852</td>\n",
       "      <td>58055.357892</td>\n",
       "      <td>403086.447567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2164.0</td>\n",
       "      <td>44831.625747</td>\n",
       "      <td>46530.124360</td>\n",
       "      <td>3.217177</td>\n",
       "      <td>14855.636205</td>\n",
       "      <td>32982.987178</td>\n",
       "      <td>58099.061334</td>\n",
       "      <td>562273.234026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1637.0</td>\n",
       "      <td>46333.091109</td>\n",
       "      <td>46556.901531</td>\n",
       "      <td>8.397256</td>\n",
       "      <td>15093.396542</td>\n",
       "      <td>33854.692454</td>\n",
       "      <td>60146.969911</td>\n",
       "      <td>394521.470373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507.0</td>\n",
       "      <td>46819.811846</td>\n",
       "      <td>50160.121468</td>\n",
       "      <td>89.464215</td>\n",
       "      <td>14728.302399</td>\n",
       "      <td>32641.000031</td>\n",
       "      <td>60193.119923</td>\n",
       "      <td>601834.998813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>45999.621937</td>\n",
       "      <td>46098.247212</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>15059.111812</td>\n",
       "      <td>33362.165177</td>\n",
       "      <td>58715.885888</td>\n",
       "      <td>428129.550783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>44479.624158</td>\n",
       "      <td>45267.767879</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>14416.613255</td>\n",
       "      <td>32699.576500</td>\n",
       "      <td>56231.869112</td>\n",
       "      <td>286769.553239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std        min           25%  \\\n",
       "features                                                                \n",
       "1         1852.0  45070.243919  44759.903113   3.238693  15074.730448   \n",
       "2         2164.0  44831.625747  46530.124360   3.217177  14855.636205   \n",
       "3         1637.0  46333.091109  46556.901531   8.397256  15093.396542   \n",
       "4         1507.0  46819.811846  50160.121468  89.464215  14728.302399   \n",
       "5         1440.0  45999.621937  46098.247212   3.238693  15059.111812   \n",
       "6         1400.0  44479.624158  45267.767879   3.238693  14416.613255   \n",
       "\n",
       "                   50%           75%            max  \n",
       "features                                             \n",
       "1         33409.543852  58055.357892  403086.447567  \n",
       "2         32982.987178  58099.061334  562273.234026  \n",
       "3         33854.692454  60146.969911  394521.470373  \n",
       "4         32641.000031  60193.119923  601834.998813  \n",
       "5         33362.165177  58715.885888  428129.550783  \n",
       "6         32699.576500  56231.869112  286769.553239  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:train_char_features.shape[1]].groupby(features['features'].str.split(' ').apply(len))['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwords = features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0', '000', '01', '01iginnted', '02', '02139', '0614g', '07', '0700', '08854', '1', '10', '100', '10003', '100125', '10036', '1007', '1011n248', '1015', '1019strategicp', '1021', '1046', '106a', '11', '110', '11012020', '1112', '1113', '113', '1140amlpm', '1194faa', '12', '120', '12026597931', '1211', '1212402', '123', '1250', '1299', '12pm', '13', '1300', '130pm', '134', '13th', '14', '1445', '14day', '1500', '1520', '15minute', '15minvte', '1616', '1640', '18', '19', '1918', '1927955', '1960', '197', '1985', '1992', '1998', '1b', '1md', '1nfecuouso1seases', '1pm', '1t', '1who', '2', '200', '2000', '20004', '20005', '2003', '2004', '2008', '200890045', '2009', '2010', '2017', '2018', '2019', '2019nco', '2019ncov', '2020', '20200303', '20200405b', '2021', '2023348387', '2024', '2031399059', '2035', '203699561', '206', '207', '208', '2089', '208922520', '212', '21205', '2123013400', '2126641289', '22', '224', '23', '230', '230pm', '235', '24', '240', '2417', '245', '25', '251028', '2520', '25m', '25minute', '26', '27', '2802926', '2803512', '2964774', '2964803', '3', '30', '300', '301', '3011983', '3014964409', '31', '310', '33', '3a', '4', '40', '400', '401', '4021663', '404', '41', '412', '41227912490', '4140521', '4155449400', '42nd', '44', '48', '5', '50', '500', '5001368', '5191205', '55', '550', '5568314', '58', '5th', '6', '60', '61', '615', '6273466', '6393286', '646754', '6467542000', '649', '6645222', '679', '693', '6all', '6j', '6l', '6pm', '7', '730am', '73223553', '751', '7a', '7a03', '7a17c', '7a17e', '7a17f', '7hs', '7th', '8', '800', '8003695815', '811', '840am', '84121', '8620312', '8am', '8d34', '8th', '9', '9086', '94105', '978', '98104', '99', '9910', '9a56', '9th', 'ab', 'abc', 'ability', 'able', 'aboard', 'abou', 'abroad', 'absence', 'absolutely', 'absorb', 'ac', 'acad', 'academies', 'academy', 'acc', 'acce', 'accelerate', 'accelerated', 'accept', 'accepted', 'acces', 'acco', 'accompanied', 'accompanying', 'according', 'accountability', 'accrue', 'accuracy', 'accuracyveracity', 'ached', 'ack', 'across', 'act', 'acti', 'acting', 'actio', 'action', 'actions', 'activ', 'active', 'activities', 'activity', 'activityworkgroup', 'acts', 'actua', 'acute', 'ad', 'adam', 'adapted', 'add', 'added', 'addis', 'additional', 'address', 'addressee', 'ade', 'adequate', 'adhanom', 'adicional', 'ading', 'adju', 'adjunct', 'adjusted', 'adm', 'admini', 'administration', 'administrations', 'administrator', 'adults', 'adva', 'advan', 'advance', 'advanced', 'advi', 'advisable', 'advise', 'advisor', 'advisors', 'advisory', 'advocating', 'aerosolization', 'aerosolized', 'affects', 'affiliate', 'afraid', 'africa', 'afte', 'afternoon', 'aga', 'agai', 'agciqcsl', 'age', 'agen', 'agence', 'agency', 'agenda', 'agent', 'agents', 'ages', 'aggressive', 'ago', 'agreed', 'agreeing', 'agreement', 'ahead', 'ahmed', 'ai', 'aid', 'aim', 'aims', 'ains', 'air', 'airborne', 'aircraft', 'airi', 'airport', 'ajlergyandinfectiousdiseases', 'ake', 'akin', 'aking', 'al', 'albert', 'alcohol', 'ale62', 'alex', 'alexand', 'alf', 'alfred', 'algorithms', 'alice', 'align', 'alle', 'aller', 'allerg', 'allison', 'allow', 'allowed', 'allowing', 'allthe', 'ally', 'almagro', 'almost', 'alone', 'alrea', 'already', 'als', 'also', 'alternifolia', 'alth', 'altho', 'alwa', 'always', 'amazon', 'amend', 'america', 'american', 'americans', 'americas', 'among', 'ample', 'anal', 'analysis', 'analyst', 'ance', 'andor', 'andrew', 'ange', 'angeles', 'anks', 'ann', 'anne', 'announce', 'announced', 'announcements', 'annua', 'ano', 'anot', 'another', 'anovaccines', 'ansa', 'answered', 'answering', 'ant', 'anthony', 'anthrax', 'antibody', 'antiv', 'antivi', 'antivira', 'antiviral', 'antivirals', 'anyo', 'anything', 'anytime', 'anyway', 'apco', 'apcowortdwide', 'aphls', 'apol', 'apolo', 'apologies', 'apologize', 'apology', 'app', 'appe', 'appearances', 'appeared', 'appears', 'appl', 'applicable', 'application', 'appointees', 'appr', 'appreciate', 'appreciated', 'appreciateyour', 'appro', 'approach', 'approaching', 'approval', 'approximates', 'apr', 'apri', 'aps', 'ar', 'arcp', 'ard', 'ardal', 'arded', 'ards', 'area', 'areas', 'ared', 'arisen', 'arkansas', 'arks', 'arly', 'arou', 'arra', 'arrange', 'arranged', 'arranging', 'arrival', 'arrive', 'arrives', 'art', 'arti', 'article', 'articles', 'artisan', 'arwady', 'aryankalayi', 'aryankalayil', 'asap', 'ascasessurge', 'ase', 'ased', 'ases', 'asf', 'ash', 'asia', 'ask', 'aske', 'asked', 'aspect', 'aspen', 'aspr', 'asprs', 'ass', 'assbackwards', 'assessed', 'assets', 'assi', 'assis', 'associate', 'association', 'assume', 'assumea', 'astho', 'astor', 'asym', 'asymp', 'asymptomatic', 'atds', 'ate', 'ated', 'ater', 'ates', 'athlet', 'athletics', 'athome', 'atic', 'ations', 'ator', 'att', 'atta', 'attached', 'attachmen', 'attachment', 'attachments', 'attack', 'attacks', 'attempt', 'attend', 'attended', 'attending', 'attention', 'attorney', 'attract', 'attributed', 'auci', 'audi', 'audiences', 'audio', 'auditorium', 'ause', 'australia', 'auth', 'autho', 'author', 'authorised', 'authorities', 'authors', 'av', 'avai', 'availability', 'available', 'ave', 'avenue', 'avoiding', 'awar', 'award', 'awardees', 'awards', 'aware', 'ay', 'aylward', 'ays', 'azar', 'azars', 'b', 'b4', 'b4l', 'b5', 'b6', 'b6cell', 'b6for', 'babu', 'baby', 'back', 'back8', 'background', 'backgrounds', 'bad', 'baldwin', 'baltimore', 'bandwidth', 'bank', 'barda', 'barie', 'barrier', 'barry', 'base', 'based', 'battifoglia', 'battles', 'bauchner', 'bcgvaccine', 'beca', 'becau', 'beco', 'becomes', 'bed', 'bee', 'befo', 'beg', 'begi', 'beh', 'beha', 'behalf', 'behalfofthe', 'behave', 'bei', 'beijing', 'bein', 'beings', 'bel', 'beli', 'believe', 'believes', 'bellevue', 'belo', 'belqxaunsgra', 'bene', 'benefit', 'benefits', 'bestowed', 'beth', 'bethe', 'bethesda', 'bett', 'bette', 'betw', 'beyond', 'bg', 'bh6', 'bid', 'biegun', 'big', 'biggest', 'billet', 'bin', 'binding', 'bio', 'bioengineering', 'biorxiv', 'biotech', 'biotechnology', 'bjj', 'blaser', 'bldg', 'ble', 'bless', 'bll', 'block', 'bloo', 'bloombe', 'bloomberg', 'blue', 'boar', 'boards', 'bob', 'body', 'boghosian', 'bookings', 'boone', 'boosting', 'borders', 'borrow', 'boston', 'bott', 'bounce', 'bout', 'brain', 'brainstorm', 'branch', 'brand', 'brandon', 'braves', 'break', 'breakfast', 'breakthroughs', 'breathing', 'breen', 'brenatecourtney', 'brett', 'brian', 'briand', 'bridge', 'brie', 'brief', 'briefing', 'briefings', 'briefly', 'brilliance', 'brin', 'bring', 'bringing', 'brittany', 'broa', 'broadcasting', 'broadly', 'broadway', 'brown', 'bruce', 'brundtlandand', 'bs', 'bsl3facility', 'bsl3samples', 'bu', 'buil', 'build', 'building', 'built', 'bureau', 'burn', 'bus', 'busi', 'busin', 'business', 'busy', 'butterf', 'butterfield', 'bx5pcp', 'c', 'ca', 'ca11er', 'cable', 'cal', 'calendar', 'california', 'call', 'callconfirmation', 'called', 'callin', 'calls', 'calm', 'cambridge', 'came', 'cameras', 'camp', 'campaign', 'canadian', 'canc', 'cance', 'canceled', 'cancer', 'candidates', 'cans', 'cans20200220b6', 'capa', 'capabilities', 'capable', 'capacity', 'capital', 'capitol', 'capture', 'car', 'cardiologist', 'care', 'career', 'careers', 'carefully', 'cari', 'caring', 'carl', 'carlos', 'carol', 'carolina', 'carter', 'casadevall', 'case', 'castro', 'catch', 'categories1', 'causing', 'caution', 'cb', 'cb6', 'cbc', 'cbh4', 'cbh6', 'cbh6j', 'cbh6l', 'cbs', 'cc', 'ccess', 'cd', 'cdc', 'cdc0d1dnceziddgmq', 'cdc0docs', 'cdcbut', 'cdcddidnceziddgmq', 'cdcddidncirdid', 'cdcemail', 'cdchad', 'cdchas', 'cdcon', 'cdcor', 'cdcs', 'cdcscientists', 'ce', 'celebrated', 'celebration', 'celestial', 'cells', 'cen', 'cent', 'cente', 'center', 'centers', 'centesfor', 'cento', 'century', 'ceo', 'ceosof', 'cepi', 'cert', 'certain', 'ces', 'cest', 'cet', 'cetand', 'ch', 'cha', 'chaffetz', 'chai', 'chair', 'chal', 'challenge', 'challenges', 'championing', 'chan', 'chance', 'chang', 'changes', 'changing', 'channel', 'channels', 'chaos', 'chapel', 'chapter', 'char', 'charac', 'characteristics', 'chat', 'chats', 'chatted', 'check', 'chf', 'chicago', 'chie', 'chief', 'chil', 'children', 'childrens', 'chin', 'chinas', 'chine', 'chinese', 'chinesecenter', 'ching', 'chno', 'chool', 'chri', 'chris', 'christie', 'chro', 'chronic', 'ci', 'cial', 'ciate', 'cies', 'cieslak', 'cine', 'circulatingair', 'circumstances', 'cities', 'citizens', 'civil', 'cl0', 'clea', 'cleaner', 'clear', 'clearance', 'clearer', 'cleverly', 'click', 'clickhere', 'clin', 'clini', 'clinic', 'clinical', 'clinically', 'clinics', 'clo', 'clos', 'closed', 'closing', 'closings', 'club', 'cme', 'cnbcbut', 'cnn', 'co', 'co80498', 'coast', 'cobj6yh', 'cobtoday', 'cochairsdr', 'code', 'coffin', 'cohen', 'cohort', 'cohorting', 'coincidence', 'coke', 'coll', 'collaboration', 'collaborators', 'colleague', 'colleagues', 'collecting', 'collection', 'colleen', 'college', 'collins', 'columbia', 'com', 'comabout', 'comdoiabs10', 'come', 'comedy', 'comes', 'comforting', 'comhealthcoronavirus', 'comjournalsjamafullarticle2764956', 'comlive', 'comm', 'comme', 'commend', 'comment', 'commenting', 'comments', 'commitment', 'committed', 'common', 'commonwealth', 'communicable', 'communicatio', 'communications', 'communicationsand', 'communit', 'community', 'comoutlookco', 'comp', 'compagno', 'company', 'companys', 'comparison', 'compassionate', 'competence', 'competitions', 'compl', 'complained', 'complementary', 'completed', 'completely', 'complex', 'complexities', 'complicated', 'composition', 'comprehensive', 'computer', 'comscholar', 'comscholarhl', 'comscholarhlenas', 'comseattlenewstimeswatchdogwhytheseattlesounders', 'comurlsatrctjqesrcssourcewebcd2ved2ahuke', 'comv2ur', 'comv2urluh', 'con', 'conc', 'conce', 'concern', 'concerned', 'concerns', 'concur', 'conference', 'conferences', 'confiden', 'confidence', 'confident', 'confidentia', 'confinned', 'confirm', 'confirmation', 'conflation', 'congressional', 'conjunctivae', 'connection', 'conrad', 'cons', 'conse', 'consequences', 'consequently', 'conserve', 'consi', 'consider', 'consideration', 'considered', 'consideri', 'considering', 'constraints', 'cont', 'contacts', 'contain', 'contained', 'containment', 'contains', 'contaminated', 'conte', 'content', 'context', 'continue', 'continued', 'continues', 'contract', 'contrast', 'contribute', 'control', 'controlthe', 'conv', 'conva', 'convenience', 'convening', 'conversation', 'convinced', 'coord', 'coordinated', 'coordinating', 'coordinator', 'cope', 'copied', 'copromot', 'copying', 'copyright', 'cornell', 'coro', 'coroii', 'coronavi', 'coronavirus', 'coronaviruscasesonboardare', 'coronov', 'corotl', 'corporation', 'corporations', 'corr', 'correo', 'correspon', 'corresponde', 'correspondence', 'correspondent', 'corticosteroid', 'cos', 'costs', 'cotrununity', 'cou', 'cough', 'coul', 'could', 'coun', 'counted', 'countermea', 'countries', 'country', 'county', 'couple', 'cour', 'cours', 'courses', 'cove', 'cover', 'coverage', 'covered', 'covering', 'covers', 'covi', 'covi0', 'covi019', 'covid', 'covid19', 'covid19advocacy', 'covid19and', 'craft', 'craighead', 'crappy', 'craziness', 'creators', 'credible', 'crew', 'cris', 'crisi', 'crisis', 'crit', 'critic', 'critical', 'croi', 'croiaudience', 'crook', 'cross', 'cruises', 'cruz', 'csis', 'csischina', 'cste', 'ction', 'ctive', 'ctory', 'ctsof', 'cu', 'cular', 'cuomo', 'cur', 'curable', 'cure', 'curious', 'curr', 'current', 'currently', 'curry', 'curtain', 'curve', 'd19', 'da', 'dail', 'dan', 'danbury', 'danger', 'daniel', 'darikradio', 'darin', 'dark', 'dat', 'data', 'date', 'dates', 'datos', 'daughte', 'davi', 'davids', 'day', 'dayhour', 'daylight', 'daylighttime', 'days', 'dc', 'ddwif', 'de', 'deadline', 'deadly', 'deal', 'dear', 'deat', 'deaths', 'debated', 'debt', 'dec', 'decade', 'december', 'decision', 'deck', 'declared', 'decline', 'decompress', 'decontamination', 'ded', 'deeply', 'default', 'defect', 'deficit', 'defined', 'definition', 'degrees', 'del', 'dela', 'delete', 'delicateness', 'delivering', 'demand', 'demands', 'demo', 'demonstration', 'dent', 'denver', 'departments', 'depending', 'deploy', 'deployed', 'der', 'derry', 'ders', 'despite', 'destinatario', 'destroy', 'deta', 'detail', 'detailed', 'details', 'dete', 'detected', 'detection', 'determined', 'dev', 'deve', 'devel', 'develop', 'developed', 'developing', 'development', 'developments', 'devi', 'device', 'devices', 'di', 'dia', 'diabetes', 'diag', 'diagnostic', 'diagnostics', 'dialin', 'diamond', 'dic', 'dical', 'dick', 'dies', 'diff', 'difference', 'differences', 'different', 'difficult', 'difficulties', 'dimensions', 'dinner', 'diplomacy', 'diplomatic', 'dire', 'direc', 'direct', 'directed', 'direction', 'directly', 'director', 'directorgeneral', 'disaster', 'disc', 'disclaim', 'disclaimer', 'disclose', 'discussing', 'discussions', 'dise', 'diseasecontrol', 'diseases', 'diseasesat', 'diseasesniaid', 'disembarkation', 'diss', 'disseminate', 'dissemination', 'distance', 'distancing', 'distress', 'distribute', 'dividing', 'divisions', 'djrectoryprofi', 'doable', 'doct', 'docu', 'document', 'documents', 'doe', 'doepel', 'doerr', 'doesnt', 'doi', 'doin', 'domestic', 'domestically', 'dona', 'donald', 'done', 'donor', 'donovan', 'dont', 'double', 'doubt', 'dozen', 'dozens', 'dphil', 'draf', 'drafted', 'dramatically', 'drastic', 'dri', 'driv', 'driver', 'droplet', 'drph', 'drs', 'drug', 'dry', 'ds', 'du', 'duals', 'dump', 'dur', 'duri', 'e', 'e5132', 'ead', 'eak', 'eaks', 'ealth', 'eard', 'earl', 'earlier', 'early', 'earlymid', 'ease', 'east', 'easte', 'eastern', 'easy', 'ebo', 'ebolalike', 'eclectic', 'econ', 'economic', 'ection', 'ects', 'ed', 'eddie', 'edicu', 'edit', 'edited', 'edu', 'edu2017', 'edu2019micro', 'edu2019storingvaccinehistoryskin1218', 'educated', 'education', 'edule', 'eed', 'eeds', 'eeks', 'een', 'effectiveness', 'efforts', 'effortsin', 'eged', 'eing', 'einstein', 'eith', 'eithe', 'eived', 'el', 'elete', 'elvander', 'ely', 'em', 'ema', 'emails', 'emer', 'emergency', 'emerging', 'emerginginchildren7027496', 'empathy', 'enact', 'enas', 'encourag', 'encourage', 'ency', 'end', 'enda', 'endorsed', 'enormously', 'enroll', 'enter', 'enthusiastic', 'enti', 'entitled', 'entrepreneurship', 'enza', 'eoc', 'eople', 'epa', 'epicenters', 'epid', 'epide', 'epidemic', 'equipped', 'er', 'eric', 'ericmstrauss', 'erika', 'ering', 'erized', 'erro', 'ers', 'erts', 'esda', 'espe', 'especially', 'ess', 'esse', 'essel', 'est', 'establis', 'establish', 'ev', 'eval', 'evalu', 'evaluated', 'evolving', 'exercise', 'existing', 'exp', 'expand', 'expansion', 'expe', 'expectations', 'exper', 'expert', 'expl', 'explored', 'expr', 'expressed', 'exte', 'extend', 'extended', 'extr', 'extra', 'extract', 'extraordinary', 'extreme', 'f', 'facebo', 'facebook', 'faci', 'facing', 'failure', 'fair', 'fake', 'fantastic', 'fatality', 'fau', 'fax', 'fe', 'fear', 'features', 'febmary', 'febr', 'february', 'fecaloral', 'federal', 'fee', 'feedback', 'feel', 'fema', 'filled', 'fina', 'finance', 'financial', 'fine', 'fir', 'firehoses', 'firs', 'first', 'firsts', 'flatten', 'flights', 'flu', 'focu', 'folkers', 'foll', 'follo', 'fore', 'forensic', 'formally', 'former', 'formulation', 'forth', 'fr', 'frie', 'friendly', 'friends', 'fro', 'fronts', 'fta9', 'fter', 'fund', 'fungal', 'futu', 'future', 'fy', 'game', 'gao', 'gavi', 'ge', 'general', 'generosity', 'generous', 'geneva', 'genevabased', 'gent', 'georgia', 'gest', 'gett', 'gf', 'gh', 'ghts', 'gies', 'giving', 'glad', 'glaringly', 'god', 'goin', 'got', 'government', 'governors', 'gram', 'grants', 'gre', 'greatng', 'greece', 'greg', 'growing', 'gues', 'guesstimate', 'guestcontact', 'guid', 'guida', 'guiderfafi', 'h6', 'ha', 'hainline', 'hall', 'hampshire', 'handle', 'handling', 'hands', 'hank', 'hanks', 'happ', 'happe', 'hardly', 'hare', 'harrisons', 'harvards', 'hats', 'havin', 'hcare', 'heads', 'heal', 'healt', 'hear', 'hehim', 'hei', 'hel', 'held', 'hemisphere', 'hence', 'hereto', 'hhsofficials', 'hi', 'highrisk', 'hiigokmt', 'hilary', 'hina', 'hite', 'hiv1', 'holding', 'holdings', 'holidays', 'home', 'hong', 'hono', 'hony', 'hool', 'hoover', 'hope', 'hopeful', 'hospital', 'hosted', 'hotel', 'hotels', 'hough', 'hought', 'housekeeping', 'howard', 'howev', 'howwhetherwhenhow', 'hq', 'hsj', 'ht', 'htipsgrant', 'htm', 'httpnews', 'httpsdoi', 'httpswww', 'hubei', 'huma', 'humanity', 'hundred', 'hynds', 'i0hih', 'i51', 'iasusa', 'iately', 'iative', 'ibly', 'ic', 'icans', 'ice', 'ich', 'icipate', 'icles', 'icu', 'icus', 'id19', 'iday', 'idays', 'ide', 'idea', 'ideas', 'iden', 'ident', 'ideo', 'ider', 'idesfightmalnutrit', 'iding', 'ie', 'iencemag', 'ient', 'ies', 'ieve', 'iew', 'iews', 'ifb', 'ified', 'ify', 'igate', 'ight', 'ihistution', 'ike', 'ill', 'illing', 'illustration', 'immunity', 'immunology', 'immunopathology', 'impact', 'impacts', 'imperative', 'implement', 'implementation', 'impo1tant', 'import', 'importantand', 'inability', 'inal', 'ince', 'incl', 'incredible', 'incredibly', 'independent', 'indian', 'indicated', 'indications', 'individual', 'individuals', 'indones', 'induced', 'industrial', 'ine', 'inf', 'infe', 'info', 'informat', 'informati', 'information', 'informed', 'ink', 'inning', 'innovations', 'inst', 'instead', 'inte', 'interes', 'interested', 'interferonbeta', 'international', 'internationally', 'internet', 'internshipresidency', 'introdu', 'introduce', 'investigation', 'invol', 'involves', 'invt', 'ions', 'ios', 'ious', 'iph', 'ipho', 'ire', 'ired', 'irst', 'iru', 'isol', 'isolating', 'issu', 'issues', 'italizat', 'iter', 'ith', 'ition', 'itionscorrect', 'itunesu', 'itutes', 'ity', 'ive', 'ived', 'ividua', 'izat', 'jacqueline', 'jama', 'jansen', 'jason', 'ject', 'jeff', 'jeffrey', 'jen', 'jennifer', 'jeremy', 'jernigan', 'jerome', 'jill', 'jm', 'joe', 'john', 'join', 'joint', 'jour', 'journal', 'jr', 'judgment', 'judi', 'jump', 'jumping', 'juncture', 'kaletra', 'kappab', 'kara', 'kat', 'kathleen', 'keen', 'kely', 'ken', 'kensington', 'kept', 'kers', 'kevin', 'keynote', 'kind', 'kindly', 'kindness', 'kit', 'kn', 'kno', 'know', 'known', 'kornel', 'kovacs', 'kraft', 'kristen', 'ks', 'l', 'lace', 'lampront', 'laoshi', 'lapoo', 'lasma', 'lated', 'lauerman', 'launch', 'lavished', 'lawrence', 'layout', 'lding', 'leadershipstudio', 'leadi', 'lear', 'lebanon', 'lee', 'left', 'legally', 'leopold', 'lescen', 'lesirfaai04032', 'lespar201', 'lesson', 'let', 'lete', 'leveraging', 'lfwe', 'lguidepa', 'li', 'liab', 'lian', 'library', 'licensed', 'licy', 'lifesupport', 'lilelynimmunitynince', 'limi', 'line', 'lines', 'ling', 'link', 'linkage', 'links', 'lipkin', 'lipsetts', 'lisa', 'lished', 'listcdc', 'listens', 'lists', 'listserv', 'liv', 'live', 'livevideo', 'liz', 'lmart', 'lmmunoregul', 'lnstttute', 'loca', 'lockdown', 'logged', 'login', 'long', 'longwe', 'loo', 'lood', 'look', 'looki', 'lopinavirritonavir', 'louder', 'louis', 'lov', 'love', 'lrastorza', 'ltd', 'luck', 'luenza', 'luhttp', 'lunar', 'lunch', 'lung', 'lways', 'lwill', 'lynns', 'machines', 'mad', 'made', 'maibox', 'majori', 'mak', 'malaysia', 'malliou', 'mana', 'management', 'managing', 'manufactures', 'many', 'marc', 'margaret', 'mariana', 'marke', 'marston', 'martin', 'mask', 'masks', 'masses', 'master', 'materials', 'matias', 'mats', 'matter', 'may', 'maybe', 'mba', 'mbio', 'mccarthy', 'mcgowan', 'mcneil', 'mea', 'meaningful', 'meas', 'mecher', 'mediately', 'medic', 'medical', 'medication', 'medicine', 'medicine20th', 'medkine', 'medline', 'mee', 'meetingcall', 'melaleuca', 'menichini', 'meone', 'mess', 'messa', 'messages', 'messaging', 'messenger', 'met', 'methods', 'mic', 'micr', 'microbiome', 'micronutrientsfor', 'mig', 'migrating', 'mill', 'milli', 'million', 'millions', 'mily', 'min', 'mind', 'minimi', 'minor', 'minu', 'minus', 'minut', 'minute', 'minutes', 'misconduct', 'misinfom1ation', 'misread', 'mitigating', 'mitigation', 'mitrecorporation', 'mix', 'mj', 'mmediate', 'mmunization', 'mnata', 'mo', 'mobi', 'mobile', 'mobilized', 'mode', 'modeled', 'modelling', 'models', 'modelsiino', 'modelstl', 'modera', 'moderator', 'modes', 'modest', 'molecular', 'moment', 'mond', 'monday', 'mong', 'monitoring', 'monoclonal', 'mont', 'months', 'mor', 'morn', 'morni', 'mornings', 'mounted', 'moves', 'moving', 'mpact', 'mpany', 'mple', 'mportat', 'mtg', 'mu', 'much', 'multiple', 'mune', 'municipa', 'muscle', 'n', 'n95', 'naccho', 'nairxsktabh', 'nal', 'name', 'narasimhan', 'nary', 'nat1on', 'nati', 'natio', 'nationalism', 'natural', 'nature', 'nced', 'ncis', 'nct', 'nder', 'ne', 'nears', 'nebulization', 'nee', 'negative', 'neighbors', 'nes', 'netw', 'networks', 'new', 'news', 'newspaper', 'nex', 'nf', 'nfaid', 'nger', 'nges', 'ni', 'nia', 'niai', 'niaidcoy', 'niaidhih', 'nic', 'nih0', 'nih000186', 'nih000187', 'nih000205', 'nih000221', 'nih000284', 'nih000383', 'nih000478', 'nih000518', 'nih000573', 'nih000575', 'nih000576', 'nih000838', 'nih000986', 'nih001037', 'nih001038', 'nih001125', 'nih001133', 'nih0012', 'nih001211', 'nih001257', 'nih001260', 'nih00138', 'nih001382', 'nih0014', 'nih001411', 'nih001434', 'nih001580', 'nih001581', 'nih001582', 'nih001583', 'nih001584', 'nih001585', 'nih001611', 'nih001620', 'nih001636', 'nih001637', 'nih001638', 'nih001659', 'nih001663', 'nih00167', 'nih001679', 'nih001683', 'nih001767', 'nih001859', 'nih001860', 'nih001874', 'nih001875', 'nih00188', 'nih001882', 'nih001911', 'nih001957', 'nih001958', 'nih001959', 'nih002008', 'nih002017', 'nih002072', 'nih002078', 'nih002079', 'nih002080', 'nih002094', 'nih002103', 'nih002247', 'nih0023', 'nihhhs', 'nihniaid', 'nihod', 'nin', 'ning', 'nion', 'nior', 'nity', 'nj', 'nk', 'nks', 'nlald', 'nlm', 'nomild', 'none', 'nonhuman', 'nonmedical', 'nonreporting', 'nonspec', 'nonspecific', 'noparalle', 'nort', 'notforprofit', 'nothing', 'noti', 'notice', 'notified', 'nown', 'np', 'ns', 'nse', 'nst', 'nt', 'ntaids', 'nter', 'nthniaid', 'ntil', 'ntly', 'nto', 'ntry', 'nts', 'ntumer', 'num', 'numbe', 'numberof', 'numbers', 'numerous', 'nurse', 'nute', 'nutrition', 'nw', 'nwd', 'nyt', 'nyu', 'oakley', 'oard', 'obal', 'objection', 'objeto', 'obtained', 'ocus', 'oevelo', 'ofcoiid19', 'offi', 'officer', 'ofo', 'ogy', 'oice', 'oijt6rea', 'oing', 'oint', 'oiseasecontro', 'oiseasecontrochina', 'oking', 'olecula', 'ome', 'onagenda', 'oncov', 'ond', 'ondemand', 'one', 'oned', 'onevaccineinjectioncouldca', 'ong', 'onli', 'onpatient', 'onse', 'ony', 'ook', 'ool', 'oom', 'oomvpdatefromchfli', 'oon', 'opdivstaffdiv', 'opdivstaffdivcore', 'ope', 'oped', 'open', 'operator', 'opin', 'opinion', 'ople', 'opportunities', 'ora', 'orde', 'orde1', 'order', 'organization', 'organized', 'organizing', 'orgcontent10', 'orgmanuscript202003', 'ori', 'orig', 'original', 'originally', 'originator', 'ork', 'orld', 'orm', 'orrow', 'orry', 'ort', 'orth', 'orthopedics', 'orts', 'ose', 'ostics', 'oth', 'ough', 'ould', 'ound', 'ount', 'oure', 'ous', 'ouse', 'outb', 'outlying', 'outpatient', 'outside', 'oved', 'ovel', 'overalmolecularaspect', 'overloaded', 'overwhelm', 'ovid', 'oviws6', 'ovlw', 'ovt6rea', 'owed', 'ower', 'ox', 'oxidase', 'pact', 'paiallels', 'painful', 'pand', 'pane', 'panelist', 'panicked', 'pape', 'paper2dnon', 'par', 'parade', 'parktime', 'particular', 'particularly', 'pas', 'passcode', 'passe', 'passengersaboardthe', 'past', 'patbrittenden', 'pate', 'path', 'pathogen', 'pathogenes', 'pathogenesi', 'patr', 'patty', 'pcr', 'peaks', 'pediatricians', 'peer', 'pennsylvania', 'peo', 'perh', 'perhaps', 'period', 'pers', 'perseverance', 'personal', 'personnel', 'persons', 'pertain', 'pete', 'peter', 'pfizer', 'ph', 'phar', 'pharm', 'phd', 'philadelphia', 'philippines', 'phys', 'physi', 'phytochemical', 'pierce', 'pies', 'ping', 'piscatawa', 'pivot', 'pivoting', 'plan', 'planproposal', 'plans', 'plasma', 'plat', 'platform', 'pnas', 'pncfu', 'pocket', 'poin', 'poli', 'policymakers', 'polio', 'poliomyeli', 'poliovirus', 'pond', 'pooled', 'popu', 'port', 'portfolio', 'portillo', 'pos', 'posed', 'posi', 'positioned', 'poss', 'possibly', 'postinfectiou', 'pote', 'potentia', 'potential', 'potentially', 'powder', 'power', 'powerless', 'ppe', 'pptx', 'ppy', 'pr', 'practice', 'practiced', 'practices', 'practitioners', 'praise', 'pravin', 'precludes', 'prefe', 'preferred', 'preliminary', 'preliminory', 'premier', 'preparation', 'prepare', 'prepared', 'preprints', 'pres', 'prescription', 'presence', 'present', 'presentations', 'presenter', 'presenting', 'preserve', 'pressreed', 'pret', 'preventable', 'prevention', 'preventive', 'previous', 'previously', 'pri', 'prin', 'princ', 'principles', 'printing', 'prioritization', 'private', 'prob', 'prod', 'produces', 'production', 'profes', 'progress', 'proh', 'prom', 'prominen', 'proofioint', 'proofpoint', 'prop', 'propagated', 'properly', 'propose', 'protocol', 'prov', 'provided', 'providers', 'proxypatient', 'prudent', 'prx', 'ps', 'pswww', 'psxovmmimlzw', 'pu', 'publish', 'published', 'pulmonary', 'pur', 'purchase', 'pursuant', 'pursue', 'pursuing', 'pushed', 'pushing', 'put', 'putting', 'py', 'pye', 'qu', 'quar', 'quara', 'quarantine', 'quarantiningon', 'queen', 'ques', 'quic', 'quotes', 'qy', 'ra', 'rabin', 'radabaugh', 'radabaughfoxnews', 'radi', 'raft', 'rage', 'rahhalmailonline', 'ralph', 'randomized', 'rap', 'raspy', 'rather', 'rationale', 'rces', 'rd', 'rday', 'rds', 'rea', 'reach', 'reached', 'reaching', 'read', 'readership', 'reading', 'readynow', 'reak', 'reaks', 'rearranged', 'reatments', 'rec', 'receive', 'recipient', 'recor', 'record', 'recorded', 'recordedupdate', 'recycled', 'redfield', 'redirection', 'redu', 'reelection', 'reemerging', 'refined', 'reflect', 'refocus', 'registered', 'reilly', 'relavant', 'rele', 'relevance', 'relevant', 'remaining', 'remains', 'remarks', 'remdesivir', 'remember', 'reminde', 'remote', 'replyall', 'reportedly', 'representat', 'representation', 'republican', 'requ', 'res', 'rese', 'researchers', 'reserve', 'resounding', 'resp', 'respects', 'respirato', 'response', 'responses', 'responsibilities', 'responsible', 'responsibly', 'responsive', 'ress', 'ressive', 'restrained', 'restricting', 'resu', 'resul', 'retu', 'revenue', 'reviewed', 'reynolds', 'rf', 'rgy', 'rhe', 'rhode', 'ri', 'rial', 'ribbon', 'ribution', 'rica', 'riences', 'ries', 'rig', 'righ', 'risdictions', 'rise', 'rite', 'rm', 'rm9a3j', 'rmal', 'rmats', 'rmed', 'rnal', 'rned', 'rner', 'robot', 'rockville', 'rok', 'role', 'rom', 'ronavirustest', 'roohi', 'rosa', 'rred', 'rror', 'rrow', 'rrymanydoses0914', 'rted', 'rter', 'rtpcrto', 'rump', 'running', 'ruocco', 'rus', 'rutgers', 'rv', 'rve', 'sa', 'sabrina', 'sabrinamalhi', 'samp', 'samples', 'sampletoanswer', 'sampling', 'samsungs', 'sapienza', 'sar', 'sara', 'saric', 'sars', 'sarsc', 'sarscov2genie', 'sarsepidemic', 'sc', 'scale', 'scenario', 'scene', 'sche', 'schiffman', 'schnitzler', 'scholar', 'schoolnik', 'schools', 'schwetz', 'scientists', 'scot', 'screening', 'sda', 'sday', 'sdt02c5q', 'se', 'sea', 'seafood', 'seasonal', 'seatt', 'seattletimes', 'sebelius', 'sec', 'secondary', 'secr', 'secretarys', 'secretion', 'section', 'sections', 'sector', 'seemed', 'seems', 'seho', 'selgrade', 'selling', 'senders', 'seni', 'sens', 'sense', 'ser', 'seri', 'series', 'seriously', 'serologies', 'serum', 'serv', 'servants', 'serve', 'sesoveral', 'session', 'sett', 'sette', 'seven', 'sf0f', 'sh', 'sha', 'shahrokh', 'shal', 'shanghai', 'shannah', 'shar', 'sharing', 'shealah', 'shed', 'shifting', 'sho', 'short', 'shortages', 'shou', 'show', 'showcase', 'signal', 'signals', 'significance', 'signoffmmwrmedia', 'simi', 'sinc', 'sing', 'single', 'sion', 'sit', 'site', 'situated', 'situation', 'sked', 'skyll', 'skyrocketed', 'slides', 'slowed', 'sm', 'smalhithehi', 'smart', 'smission', 'sneeze', 'sobering', 'social', 'societyusa', 'solutions', 'som', 'somehow', 'sommer', 'sonal', 'sooner', 'sor', 'sorry', 'sort', 'sour', 'sp', 'space', 'spans', 'spe', 'specialists', 'specialized', 'specializesin', 'spokewith', 'sponsor', 'spook', 'sporting', 'sports', 'spre', 'spring', 'srnalhithehill', 'srnarasimhancell', 'ssed', 'sses', 'ssions', 'sta', 'stable', 'staff', 'staiie', 'stakeholders', 'staley', 'stan', 'standardsbased', 'standardsthe', 'stanford', 'star', 'started', 'statements', 'states2020030sa6ced5aa', 'status', 'stay', 'staying', 'stays', 'steal', 'sted', 'stem', 'stenson', 'stephen', 'ster', 'stered', 'steroids', 'stic', 'sto', 'stop', 'stor', 'str', 'strategic', 'stre', 'stream', 'streamed', 'stress', 'stretched', 'stri', 'strictly', 'stry', 'sts', 'stt', 'stu', 'stud', 'studies', 'studio', 'studynovel', 'subj', 'subs', 'subside', 'successfu', 'suggestionsadd', 'suit', 'suite', 'suject', 'summ', 'summary', 'summit', 'sund', 'sup', 'supe', 'supplier', 'sur', 'surg', 'surge', 'surrogate', 'surv', 'surveillance', 'survivors', 'sustained', 'swab', 'swabs', 'swer', 'swiftly', 'swww', 'sy', 'syllabusx', 'sylvie', 'symbo', 'symposium', 'symptomatic', 'symptoms', 'sys', 'syst', 'systems', 'systemsorien', 'tabak', 'tact', 'taff', 'tain', 'take', 'talent', 'tam', 'tami', 'tand', 'tandfonline', 'tanding', 'tao', 'tary', 'task', 'tat', 'tate', 'tbis', 'team', 'tech', 'technical', 'tect', 'tedros', 'tel', 'ten', 'tentat', 'territories', 'terror', 'tes', 'test', 'testimony', 'testing', 'texas', 'thanksand', 'thatd', 'thedoc', 'thera', 'therapeut', 'therapeutic', 'therapies31hcl', 'therein', 'thermal', 'thes', 'thin', 'thinking', 'thirdparty', 'thisand', 'thou', 'though', 'thought', 'thre', 'three', 'thtsolitovima', 'thursday', 'thus', 'ti', 'tial', 'ticle', 'tics', 'tify', 'tighte', 'tiles', 'till', 'timelines', 'timeproven', 'times', 'ting', 'tip', 'tis', 'tissues', 'title', 'titles', 'tnfalpha', 'tnihumans', 'tobias', 'toda', 'tol', 'toll', 'tomorrow', 'toms', 'tool', 'tors', 'tory', 'tournament', 'tovima', 'towa', 'toward', 'towards', 'tower', 'town', 'tps', 'tra', 'tracked', 'transcripts', 'transmi', 'transmissibili', 'transmissionis', 'tree', 'tremendously', 'tri', 'tribal', 'trick', 'tried', 'trimming', 'tro', 'trol', 'troye', 'trum', 'tted', 'tter', 'ttps', 'ttps3a', 'tua', 'tual', 'tudy', 'tues', 'tuler', 'ture', 'turn', 'tute', 'tv', 'twostep', 'u8zi', 'uae', 'uals', 'uary', 'uble', 'udies', 'ue', 'uest', 'uestion', 'ugh', 'ught', 'uhan', 'uk', 'ukacutecareexclusivenationalalertascoronavirusrelatedconditionmaybe', 'ully', 'ults', 'uman', 'umber', 'umsom', 'unauthor', 'unauthorized', 'unburden', 'uncomfortable', 'uncommon', 'und', 'underestimating', 'undergo', 'unders', 'understand', 'underway', 'unhelpful', 'uni', 'unique', 'unit', 'united', 'univ', 'univers', 'university', 'universityin', 'unlawfu', 'unmet', 'unpublished', 'unsett', 'unsure', 'uoxouwu', 'upda', 'upon', 'upper', 'ur', 'urce', 'ures', 'urgency', 'urldcfense', 'urse', 'us', 'usage', 'use', 'uses', 'usin', 'usuallythe', 'ut', 'ute', 'utes', 'utiv', 'vac', 'vaccination', 'vaccine', 'vaccineopv', 'vaccines', 'vaccinologists', 'varadara', 'variety', 'various', 'vectors', 'ved', 'vel', 'vely', 'ven', 'vent', 'ver', 'verification', 'vermont', 'vero', 'version', 'versions', 'versusevacuatingthem', 'vetera', 'vi', 'via', 'vid', 'vide', 'vietnam', 'vima', 'ving', 'vir', 'virology', 'virtual', 'viru', 'virusinduced', 'visiting', 'visitwhoever', 'vite', 'vitro', 'voic', 'vp', 'wait', 'walmart', 'ware', 'warn', 'warning', 'warren', 'wash', 'ways', 'web', 'webbased', 'website', 'wee', 'week', 'weekend', 'wel', 'wellpowered', 'wgbh', 'wha', 'whats', 'whet', 'wheth', 'whi', 'whic', 'whit', 'wholeheartedly', 'wide', 'widely', 'widescale', 'william', 'wing', 'wis', 'wit', 'within', 'without', 'wlf', 'wn', 'wnch111e', 'worke', 'workfor', 'worki', 'works', 'workshop', 'world', 'worry', 'worse', 'worst', 'woul', 'wpierceapcoworldwide', 'wu', 'wuha', 'ww', 'www', 'wytik', 'x', 'xi', 'year', 'yet', 'yh', 'yikes', 'ying', 'yo', 'yone', 'yor', 'youd', 'za', 'zac', 'ze', 'zebley', 'zhong', 'zhu', 'zika', 'zing', 'zuccarofoxnews', 'zuck'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "cv_score = np.mean(cross_val_score(reg, train_features, y_train.values, cv=3, scoring='r2'))\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17847"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[train_char_features.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37847, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    6,     7,    10,    11,    12,    19,    20,    21,    22,\n",
       "               23,\n",
       "            ...\n",
       "            37833, 37834, 37836, 37838, 37840, 37841, 37842, 37844, 37845,\n",
       "            37846],\n",
       "           dtype='int64', length=17478)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[weights.between(-weights.std()*0.3, weights.std()*0.3)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.025804388020673974"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train[class_name]\n",
    "classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "\n",
    "\n",
    "scores.append(cv_score)\n",
    "print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "classifier.fit(train_features, train_target)\n",
    "submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for class_name in class_names:\n",
    "    train_target = train[class_name]\n",
    "    classifier = LogisticRegression(C=0.1, solver='sag')\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print('CV score for class {} is {}'.format(class_name, cv_score))\n",
    "\n",
    "    classifier.fit(train_features, train_target)\n",
    "    submission[class_name] = classifier.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=30000, ngram_range=(1, 4), stop_words='english',\n",
       "                strip_accents='unicode', sublinear_tf=True,\n",
       "                token_pattern='\\\\w{1,}')"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "# train_text = train['comment_text']\n",
    "# test_text = test['comment_text']\n",
    "# all_text = pd.concat([train_text, test_text])\n",
    "all_text = df['text']\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=30000)\n",
    "word_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', max_features=20000, ngram_range=(2, 6),\n",
       "                stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=True)"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=20000)\n",
    "char_vectorizer.fit(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0 2035',\n",
       " '0 2035 251028',\n",
       " '0 2035 251028 cell',\n",
       " '0 5',\n",
       " '0 b6',\n",
       " '00',\n",
       " '000',\n",
       " '000 000',\n",
       " '000 remain',\n",
       " '000 remain available',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '0mb',\n",
       " '1',\n",
       " '1 000',\n",
       " '1 1',\n",
       " '1 1 1',\n",
       " '1 1 1 1',\n",
       " '1 2',\n",
       " '1 2020',\n",
       " '1 58',\n",
       " '1 58 22',\n",
       " '1 58 22 07',\n",
       " '1 8',\n",
       " '1 8cineole',\n",
       " '1 clinical',\n",
       " '1 clinical trial',\n",
       " '1 pm',\n",
       " '1 pm tomorrow',\n",
       " '1 room',\n",
       " '1 room 138',\n",
       " '1 room 138 p',\n",
       " '10',\n",
       " '10 minutes',\n",
       " '100',\n",
       " '1002',\n",
       " '1002cbdv',\n",
       " '10036',\n",
       " '1007',\n",
       " '1007 stephen',\n",
       " '1007 stephen morrison',\n",
       " '1007 stephen morrison smorrisocs',\n",
       " '101',\n",
       " '1015',\n",
       " '1015 minute',\n",
       " '11',\n",
       " '11 12',\n",
       " '11012020',\n",
       " '113',\n",
       " '114',\n",
       " '1142',\n",
       " '115',\n",
       " '119',\n",
       " '12',\n",
       " '12 2020',\n",
       " '120',\n",
       " '120 pm',\n",
       " '1200',\n",
       " '121',\n",
       " '1210',\n",
       " '1210 pm',\n",
       " '123',\n",
       " '12pm',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '130pm',\n",
       " '138',\n",
       " '138 p',\n",
       " '138 p b6',\n",
       " '138 p b6 m',\n",
       " '14',\n",
       " '14 days',\n",
       " '14day',\n",
       " '14day period',\n",
       " '15',\n",
       " '15 2020',\n",
       " '15 minute',\n",
       " '15 minutes',\n",
       " '150',\n",
       " '1500',\n",
       " '1520',\n",
       " '1520 minutes',\n",
       " '1520 minutes slides',\n",
       " '1520 minutes slides state',\n",
       " '15minute',\n",
       " '15minute update',\n",
       " '15minute update 3',\n",
       " '15minute update 3 focuson',\n",
       " '16',\n",
       " '16 2020',\n",
       " '16 great',\n",
       " '16 great marlborough',\n",
       " '16 great marlborough street',\n",
       " '1600',\n",
       " '1615',\n",
       " '162',\n",
       " '17',\n",
       " '17 2020',\n",
       " '18',\n",
       " '18 2020',\n",
       " '180',\n",
       " '182',\n",
       " '19',\n",
       " '19 2020',\n",
       " '19 pandemic',\n",
       " '19 tue',\n",
       " '1918',\n",
       " '1918 flu',\n",
       " '1918 flu pandemic',\n",
       " '1927955',\n",
       " '1927955 turner',\n",
       " '1927955 turner entertainment',\n",
       " '1927955 turner entertainment networks',\n",
       " '197',\n",
       " '1970s',\n",
       " '1980s',\n",
       " '1984',\n",
       " '1985',\n",
       " '1992',\n",
       " '1998',\n",
       " '1b',\n",
       " '2',\n",
       " '2 2020',\n",
       " '2 2020 1007',\n",
       " '2 2020 1007 stephen',\n",
       " '2 3',\n",
       " '2 3 hour',\n",
       " '2 37',\n",
       " '2 5',\n",
       " '2 covid',\n",
       " '2 covid 19',\n",
       " '2 weeks',\n",
       " '2 weeks grateful',\n",
       " '2 weeks grateful know',\n",
       " '2 years',\n",
       " '20',\n",
       " '20 2020',\n",
       " '20 covid19',\n",
       " '20 covid19 event',\n",
       " '20 covid19 event seek',\n",
       " '20 work',\n",
       " '20 work best',\n",
       " '20 work best steve',\n",
       " '20 years',\n",
       " '200',\n",
       " '200 independence',\n",
       " '2000',\n",
       " '20005',\n",
       " '2001',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '20172018',\n",
       " '2018',\n",
       " '2019',\n",
       " '2019 ncov',\n",
       " '2019 novel',\n",
       " '2019 novel coronavirus',\n",
       " '2019 novel coronavirus teleconference',\n",
       " '2019nco',\n",
       " '2019nco v',\n",
       " '2019ncov',\n",
       " '202',\n",
       " '2020',\n",
       " '2020 1002',\n",
       " '2020 1007',\n",
       " '2020 1007 stephen',\n",
       " '2020 1007 stephen morrison',\n",
       " '2020 5',\n",
       " '2020 751',\n",
       " '2020 geneva',\n",
       " '2020 national',\n",
       " '2020 national institutes',\n",
       " '2020 national institutes health',\n",
       " '2020 t',\n",
       " '20200405b',\n",
       " '20201',\n",
       " '2021',\n",
       " '2023348387',\n",
       " '2023348387 c',\n",
       " '2024',\n",
       " '2024 prevent',\n",
       " '2024 prevent prepare',\n",
       " '2035',\n",
       " '2035 251028',\n",
       " '2035 251028 cell',\n",
       " '206',\n",
       " '207',\n",
       " '207 693',\n",
       " '20850',\n",
       " '20892',\n",
       " '20892 b6',\n",
       " '20892 b6 3014964409',\n",
       " '20892 b6 3014964409 fax',\n",
       " '20892 b6 media',\n",
       " '20892 b6 media line',\n",
       " '20892 direct',\n",
       " '20892 direct b',\n",
       " '20892 phone',\n",
       " '20892 phone b6',\n",
       " '208922520',\n",
       " '208922520 phone',\n",
       " '208922520 phone b',\n",
       " '208922520 phone b 6',\n",
       " '208922520 phone b6',\n",
       " '208922520 phone b6 fax',\n",
       " '21',\n",
       " '212',\n",
       " '2126641289',\n",
       " '2126641289 c',\n",
       " '2126641289 c b6',\n",
       " '21st',\n",
       " '22',\n",
       " '22 07',\n",
       " '22 2020',\n",
       " '220',\n",
       " '223',\n",
       " '22520',\n",
       " '23',\n",
       " '23 2020',\n",
       " '230',\n",
       " '230pm',\n",
       " '24',\n",
       " '24 2020',\n",
       " '240',\n",
       " '245',\n",
       " '247',\n",
       " '25',\n",
       " '25 2020',\n",
       " '25 years',\n",
       " '250',\n",
       " '251028',\n",
       " '251028 cell',\n",
       " '2520',\n",
       " '2520 bethesda',\n",
       " '2520 bethesda md',\n",
       " '2520 national',\n",
       " '2520 national institutes',\n",
       " '2520 national institutes health',\n",
       " '2520 room',\n",
       " '2520 room 7a03',\n",
       " '2520 room 7a03 bethesda',\n",
       " '26',\n",
       " '26 2020',\n",
       " '27',\n",
       " '27 2020',\n",
       " '28',\n",
       " '28 2020',\n",
       " '2802926',\n",
       " '2802926 cable',\n",
       " '2802926 cable news',\n",
       " '2802926 cable news international',\n",
       " '2803512',\n",
       " '2803512 turner',\n",
       " '2803512 turner broadcasting',\n",
       " '2803512 turner broadcasting holdings',\n",
       " '29',\n",
       " '29 2020',\n",
       " '2nd',\n",
       " '2nd floor',\n",
       " '3',\n",
       " '3 2020',\n",
       " '3 2020 national',\n",
       " '3 2020 national institutes',\n",
       " '3 focuson',\n",
       " '3 hour',\n",
       " '3 queen',\n",
       " '3 queen victoria',\n",
       " '3 queen victoria street',\n",
       " '30',\n",
       " '30 2020',\n",
       " '30 2020 1002',\n",
       " '30 2024',\n",
       " '30 2024 prevent',\n",
       " '30 2024 prevent prepare',\n",
       " '30 days',\n",
       " '30 minutes',\n",
       " '300',\n",
       " '301',\n",
       " '301 4021663',\n",
       " '301 4964409',\n",
       " '301 4964409 email',\n",
       " '301 4964409 email b',\n",
       " '301 4964409 email b6',\n",
       " '3014964409',\n",
       " '3014964409 fax',\n",
       " '31',\n",
       " '31 center',\n",
       " '31 center drive',\n",
       " '31 center drive msc',\n",
       " '31 center drive room',\n",
       " '31 room',\n",
       " '31 room 7a03',\n",
       " '31 room 7a03 31',\n",
       " '32',\n",
       " '33',\n",
       " '34',\n",
       " '35',\n",
       " '37',\n",
       " '4',\n",
       " '4 1',\n",
       " '4 1 58',\n",
       " '4 b',\n",
       " '4 b 4',\n",
       " '4 b 5',\n",
       " '4 best',\n",
       " '4 thank',\n",
       " '4 thanks',\n",
       " '40',\n",
       " '40 degrees',\n",
       " '400',\n",
       " '401',\n",
       " '402',\n",
       " '4021663',\n",
       " '41',\n",
       " '41227912490',\n",
       " '42nd',\n",
       " '43',\n",
       " '44',\n",
       " '44 0',\n",
       " '44 0 2035',\n",
       " '45',\n",
       " '47',\n",
       " '48',\n",
       " '48 hours',\n",
       " '4964409',\n",
       " '4964409 email',\n",
       " '4964409 email b',\n",
       " '4964409 email b 6',\n",
       " '4964409 email b6',\n",
       " '4964409 email b6 information',\n",
       " '5',\n",
       " '5 2020',\n",
       " '5 b',\n",
       " '5 b 5',\n",
       " '5 b5',\n",
       " '5 best',\n",
       " '5 let',\n",
       " '5 let know',\n",
       " '5 min',\n",
       " '5 thank',\n",
       " '5 thanks',\n",
       " '5 years',\n",
       " '50',\n",
       " '50 million',\n",
       " '50 million people',\n",
       " '500',\n",
       " '5001368',\n",
       " '5001368 incorporated',\n",
       " '5001368 incorporated england',\n",
       " '5001368 incorporated england wales',\n",
       " '51',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '58',\n",
       " '58 22',\n",
       " '58 22 07',\n",
       " '59',\n",
       " '5l',\n",
       " '5th',\n",
       " '6',\n",
       " '6 b',\n",
       " '6 b 5',\n",
       " '6 b6',\n",
       " '6 cell',\n",
       " '6 cell b',\n",
       " '6 email',\n",
       " '6 fax',\n",
       " '6 fax 301',\n",
       " '6 fax 301 4964409',\n",
       " '6 information',\n",
       " '6 information email',\n",
       " '6 information email attachments',\n",
       " '6 mobile',\n",
       " '6 voice',\n",
       " '6 wrote',\n",
       " '60',\n",
       " '60 million',\n",
       " '60 million people',\n",
       " '61',\n",
       " '615',\n",
       " '62',\n",
       " '63',\n",
       " '65',\n",
       " '659',\n",
       " '68',\n",
       " '69',\n",
       " '693',\n",
       " '6j',\n",
       " '6l',\n",
       " '6pm',\n",
       " '7',\n",
       " '7 2020',\n",
       " '7 a03',\n",
       " '7 hour',\n",
       " '70',\n",
       " '700',\n",
       " '710',\n",
       " '72',\n",
       " '73',\n",
       " '730am',\n",
       " '749',\n",
       " '751',\n",
       " '76',\n",
       " '79',\n",
       " '7a03',\n",
       " '7a03 31',\n",
       " '7a03 31 center',\n",
       " '7a03 31 center drive',\n",
       " '7a03 bethesda',\n",
       " '7a03 bethesda maryland',\n",
       " '7a03 bethesda maryland 20892',\n",
       " '7a17c',\n",
       " '7a17c bethesda',\n",
       " '7a17c bethesda md',\n",
       " '7a17c bethesda md 20892',\n",
       " '7a17e',\n",
       " '7a17e bethesda',\n",
       " '7a17e bethesda md',\n",
       " '7a17e bethesda md 20892',\n",
       " '7a17f',\n",
       " '7hs',\n",
       " '7th',\n",
       " '8',\n",
       " '8 2020',\n",
       " '80',\n",
       " '800',\n",
       " '82',\n",
       " '83',\n",
       " '85',\n",
       " '86',\n",
       " '89',\n",
       " '8cineole',\n",
       " '8th',\n",
       " '9',\n",
       " '9 2020',\n",
       " '90',\n",
       " '900',\n",
       " '910',\n",
       " '930am',\n",
       " '95',\n",
       " '99',\n",
       " 'a03',\n",
       " 'ab',\n",
       " 'abc',\n",
       " 'abc news',\n",
       " 'ability',\n",
       " 'able',\n",
       " 'able accept',\n",
       " 'able accept invitation',\n",
       " 'able contain',\n",
       " 'able contain outbreak',\n",
       " 'able contain outbreak possible',\n",
       " 'able detect',\n",
       " 'able join',\n",
       " 'able provide',\n",
       " 'abo',\n",
       " 'abo u',\n",
       " 'abo u t',\n",
       " 'abo ut',\n",
       " 'abou',\n",
       " 'abou t',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'absorb',\n",
       " 'absorption',\n",
       " 'abutaleb',\n",
       " 'abutaleb washington',\n",
       " 'abutaleb washington post',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academies',\n",
       " 'academy',\n",
       " 'academy medicine',\n",
       " 'academy microbiology',\n",
       " 'acc',\n",
       " 'accelerate',\n",
       " 'accelerated',\n",
       " 'accelerated early',\n",
       " 'accelerated early work',\n",
       " 'accelerated early work vaccines',\n",
       " 'accelerating',\n",
       " 'accept',\n",
       " 'accept invitation',\n",
       " 'accept liability',\n",
       " 'accept liability statement',\n",
       " 'accept liability statement senders',\n",
       " 'accept liability statements',\n",
       " 'accept liability statements sender',\n",
       " 'accept liability statements senders',\n",
       " 'accepted',\n",
       " 'access',\n",
       " 'access code',\n",
       " 'accessible',\n",
       " 'accompanied',\n",
       " 'accompanying',\n",
       " 'according',\n",
       " 'according mr',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'accuracy',\n",
       " 'accuracy veracity',\n",
       " 'accuracy veracity does',\n",
       " 'accuracyveracity',\n",
       " 'accuracyveracity does',\n",
       " 'accuracyveracity does necessarily',\n",
       " 'accurate',\n",
       " 'ace2',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'acid',\n",
       " 'acs',\n",
       " 'act',\n",
       " 'act 1985',\n",
       " 'act ion',\n",
       " 'acting',\n",
       " 'acting director',\n",
       " 'acting director ninr',\n",
       " 'acting director ninr associate',\n",
       " 'action',\n",
       " 'action date',\n",
       " 'action reliance',\n",
       " 'action reliance information',\n",
       " 'action reliance information persons',\n",
       " 'actions',\n",
       " 'activ',\n",
       " 'activ leadership',\n",
       " 'activ leadership meeting',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'activities essential',\n",
       " 'activity',\n",
       " 'activity type',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acute',\n",
       " 'acute respiratory',\n",
       " 'acute respiratory distress',\n",
       " 'acute respiratory distress syndrome',\n",
       " 'acute respiratory syndrome',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adapted',\n",
       " 'add',\n",
       " 'add itional',\n",
       " 'add panel',\n",
       " 'add panel epicenters',\n",
       " 'add panel epicenters italy',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addit',\n",
       " 'addit ion',\n",
       " 'addition',\n",
       " 'addition niaid',\n",
       " 'additional',\n",
       " 'additional information',\n",
       " 'additional n',\n",
       " 'additional n ational',\n",
       " 'additionally',\n",
       " 'address',\n",
       " 'address b6',\n",
       " 'address covid19',\n",
       " 'address ing',\n",
       " 'address overall',\n",
       " 'address overall plan',\n",
       " 'address overall plan discuss',\n",
       " 'addressed',\n",
       " 'addressed contain',\n",
       " 'addressed contain confidential',\n",
       " 'addressee',\n",
       " 'addressee addressee',\n",
       " 'addressee addressee indicated',\n",
       " 'addressee addressee indicated message',\n",
       " 'addressee copy',\n",
       " 'addressee indicated',\n",
       " 'addressee indicated message',\n",
       " 'addressee indicated message responsible',\n",
       " 'addressee views',\n",
       " 'addressee views expressed',\n",
       " 'addressee views expressed official',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adequate',\n",
       " 'adherence',\n",
       " 'adjusted',\n",
       " 'adm',\n",
       " 'admin',\n",
       " 'administ',\n",
       " 'administered',\n",
       " 'administration',\n",
       " 'administration say',\n",
       " 'administrations',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admission',\n",
       " 'admitted',\n",
       " 'adobe',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'adults united',\n",
       " 'adults united states',\n",
       " 'adults united states safe',\n",
       " 'adv',\n",
       " 'advance',\n",
       " 'advance team',\n",
       " 'advanced',\n",
       " 'advice',\n",
       " 'advice amend',\n",
       " 'advice amend layout',\n",
       " 'advice amend layout event',\n",
       " 'advisable',\n",
       " 'advisable request',\n",
       " 'advisable request forward',\n",
       " 'advisable request forward secy',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advisory',\n",
       " 'advisory board',\n",
       " 'advisory panel',\n",
       " 'advocacy',\n",
       " 'advocating',\n",
       " 'aerosol',\n",
       " 'aerosolized',\n",
       " 'affairs',\n",
       " 'affairs office',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affects',\n",
       " 'affiliate',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'afte',\n",
       " 'afternoon',\n",
       " 'afternoon evening',\n",
       " 'afternoon evening east',\n",
       " 'afternoon evening east coast',\n",
       " 'aga',\n",
       " 'agains',\n",
       " 'agains t',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agenda callin',\n",
       " 'agenda callin details',\n",
       " 'agent',\n",
       " 'agent responsible',\n",
       " 'agent responsible delivering',\n",
       " 'agent responsible delivering message',\n",
       " 'agents',\n",
       " 'aggressive',\n",
       " 'aggressive testing',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreed participate',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'ahead',\n",
       " 'ahmed',\n",
       " 'aids',\n",
       " 'ail',\n",
       " 'aim',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'airborne',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'airi',\n",
       " 'airlines',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'akaike',\n",
       " 'akaike t',\n",
       " 'al',\n",
       " 'alan',\n",
       " 'albert',\n",
       " 'alcohol',\n",
       " 'alert',\n",
       " 'alert free',\n",
       " 'alert free use',\n",
       " 'alert free use public',\n",
       " 'alessandra',\n",
       " 'alessandra lucivero',\n",
       " 'alex',\n",
       " 'alex azar',\n",
       " 'alexander',\n",
       " 'alexandra',\n",
       " 'alfonso',\n",
       " 'alfred',\n",
       " 'alice',\n",
       " 'alice park',\n",
       " 'alice parktime',\n",
       " 'alice parktime com',\n",
       " 'alice parktime com aliceparkny',\n",
       " 'aliceparkny',\n",
       " 'alison',\n",
       " 'allergy',\n",
       " 'allergy infect',\n",
       " 'allergy infect ious',\n",
       " 'allergy infectious',\n",
       " 'allergy infectious diseases',\n",
       " 'allergy infectious diseases build',\n",
       " 'allergy infectious diseases building',\n",
       " 'allergy infectious diseases national',\n",
       " 'allergy infectious diseases niaid',\n",
       " 'allergy infectious diseases shall',\n",
       " 'allergy nd',\n",
       " 'allergy nd infectious',\n",
       " 'allergy nd infectious diseases',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allthe',\n",
       " 'allthe best',\n",
       " 'almagro',\n",
       " 'almagro 3',\n",
       " 'alpha1',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternifolia',\n",
       " 'alternifolia tea',\n",
       " 'alternifolia tea tree',\n",
       " 'alternifolia tea tree oil',\n",
       " 'alveolar',\n",
       " 'alzheimers',\n",
       " 'ama',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'ame',\n",
       " 'amelie',\n",
       " 'amelie rioux',\n",
       " 'amelie rioux b6',\n",
       " 'amend',\n",
       " 'amend layout',\n",
       " 'amend layout event',\n",
       " 'amend layout event including',\n",
       " 'amer',\n",
       " 'america',\n",
       " 'american',\n",
       " 'american academy',\n",
       " 'american academy microbiology',\n",
       " 'american college',\n",
       " 'american lung',\n",
       " 'american lung association',\n",
       " 'american masters',\n",
       " 'american people',\n",
       " 'american public',\n",
       " 'american society',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'amounts',\n",
       " 'ample',\n",
       " 'amy',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analyst special',\n",
       " 'analyst special assistant',\n",
       " 'analyst special assistant director',\n",
       " 'analytics',\n",
       " 'ance',\n",
       " 'ance high',\n",
       " 'ance high dear',\n",
       " 'ance high dear dr',\n",
       " 'anderson',\n",
       " 'andor',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'andy',\n",
       " 'anecdotal',\n",
       " 'angeles',\n",
       " 'angiotensin',\n",
       " 'animal',\n",
       " 'animal models',\n",
       " 'animals',\n",
       " 'ann',\n",
       " 'anne',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annua',\n",
       " 'annua l',\n",
       " 'annual',\n",
       " 'ano',\n",
       " 'ans',\n",
       " 'ansa',\n",
       " 'ansa science',\n",
       " 'ansa science web',\n",
       " 'ansa science web page',\n",
       " 'answer',\n",
       " 'answer questions',\n",
       " 'answer questions look',\n",
       " 'answer questions look forward',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anthony',\n",
       " 'anthony fauci',\n",
       " 'anthony fauci director',\n",
       " 'anthony fauci director national',\n",
       " 'anthony fauci m',\n",
       " 'anthony fauci m d',\n",
       " 'anthony foxx',\n",
       " 'anthony nihniaid',\n",
       " 'anthony nihniaid e',\n",
       " 'anthony nihniaid e b',\n",
       " 'anthony nihniaid e b6',\n",
       " 'anthony nihniaid e subject',\n",
       " 'anthony s',\n",
       " 'anthony s fauci',\n",
       " 'anthony s fauci director',\n",
       " 'anthony s fauci m',\n",
       " 'anthony s fauci md',\n",
       " 'anthrax',\n",
       " 'anti',\n",
       " 'antibodies',\n",
       " 'antibody',\n",
       " 'anticipate',\n",
       " 'anticipated',\n",
       " 'antigen',\n",
       " 'antimicrobial',\n",
       " 'antivira',\n",
       " 'antivira l',\n",
       " 'antiviral',\n",
       " 'antiviral activities',\n",
       " 'antiviral activity',\n",
       " 'antivirals',\n",
       " 'antivirals wi',\n",
       " 'antivirals wi ll',\n",
       " 'antivirals wi ll panel',\n",
       " 'antonio',\n",
       " 'anyth',\n",
       " 'anyth ing',\n",
       " 'anytime',\n",
       " 'aol',\n",
       " 'apco',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'apologize advance',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appear',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'applicable',\n",
       " 'applicable law',\n",
       " 'application',\n",
       " 'applied',\n",
       " 'apply',\n",
       " 'apprec',\n",
       " 'apprec iate',\n",
       " 'appreciat',\n",
       " 'appreciate',\n",
       " 'appreciate confirm',\n",
       " 'appreciated',\n",
       " 'appreciated kindly',\n",
       " 'appreciated kindly confirm',\n",
       " 'appreciated kindly confirm participation',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropr',\n",
       " 'appropriate',\n",
       " 'appropriated',\n",
       " 'appropriated title',\n",
       " 'appropriations',\n",
       " 'approval',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'approximately',\n",
       " 'apr',\n",
       " 'apr il',\n",
       " 'april',\n",
       " 'april 1',\n",
       " 'april 19',\n",
       " 'april 28',\n",
       " 'april 3',\n",
       " 'april 8',\n",
       " 'april 8 2020',\n",
       " 'ar',\n",
       " 'arb',\n",
       " 'arbs',\n",
       " 'arch',\n",
       " 'ard',\n",
       " 'arded',\n",
       " 'ards',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'arguments',\n",
       " 'ari',\n",
       " 'ari melber',\n",
       " 'ari pestering',\n",
       " 'arizona',\n",
       " 'arkansas',\n",
       " 'arlington',\n",
       " 'arm',\n",
       " 'army',\n",
       " 'arose',\n",
       " 'arrange',\n",
       " 'arrange time',\n",
       " 'arranged',\n",
       " 'array',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'art',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artists',\n",
       " 'arturo',\n",
       " 'arturo casadevall',\n",
       " 'arturo casadevall md',\n",
       " 'arturo casadevall md phd',\n",
       " 'asap',\n",
       " 'asf',\n",
       " 'ash',\n",
       " 'asia',\n",
       " 'asia group',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'ask 1',\n",
       " 'ask question',\n",
       " 'ask questions',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'asm',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspen',\n",
       " 'aspen institute',\n",
       " 'aspr',\n",
       " 'assays',\n",
       " 'assessed',\n",
       " 'assessment',\n",
       " 'assets',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assistant director',\n",
       " 'assistant director national',\n",
       " 'assistant director national institute',\n",
       " 'assistant professor',\n",
       " 'assistant secretary',\n",
       " 'assistant secretary health',\n",
       " 'assistant secretary preparedness',\n",
       " 'assistant secretary preparedness response',\n",
       " 'associate',\n",
       " 'associate deputy',\n",
       " 'associate deputy director',\n",
       " 'associate deputy director nih',\n",
       " 'associate director',\n",
       " 'associate editor',\n",
       " 'associate editor ithe',\n",
       " 'associate editor ithe hill',\n",
       " 'associate professor',\n",
       " 'associated',\n",
       " 'associates',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'astani',\n",
       " ...]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-424-365341e94809>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wprds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1198\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "counts = ngram_vectorizer.fit_transform([['words', 'wprds']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' w',\n",
       " ' wo',\n",
       " ' wor',\n",
       " ' wp',\n",
       " ' wpr',\n",
       " 'd',\n",
       " 'ds',\n",
       " 'ds ',\n",
       " 'o',\n",
       " 'or',\n",
       " 'ord',\n",
       " 'ords',\n",
       " 'p',\n",
       " 'pr',\n",
       " 'prd',\n",
       " 'prds',\n",
       " 'r',\n",
       " 'rd',\n",
       " 'rds',\n",
       " 'rds ',\n",
       " 's',\n",
       " 's ',\n",
       " 'w',\n",
       " 'wo',\n",
       " 'wor',\n",
       " 'word',\n",
       " 'wp',\n",
       " 'wpr',\n",
       " 'wprd']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ngram_vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-417-48d1a9565e1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mngram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "ngram_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-409-039e86f60817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size), \\\n\u001b[0;32m      7\u001b[0m                                                  tokenizer=TreebankWordTokenizer().tokenize)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{1}-grams: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m         \"\"\"\n\u001b[0;32m   1294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m         return [t for t, i in sorted(self.vocabulary_.items(),\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "ngram_size = 4\n",
    "string = [\"I really like python, it's pretty awesome.\"]\n",
    "vect = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(ngram_size,ngram_size), \\\n",
    "                                                 tokenizer=TreebankWordTokenizer().tokenize)\n",
    "print('{1}-grams: {0}'.format(vect.get_feature_names(), ngram_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
