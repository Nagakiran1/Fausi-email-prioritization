{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build an email prioritization model\n",
    "\n",
    "Priority can be defined as how quick an email needed to be dealt with, the content of the email, how quickly it was responded to etc. There's no ground truth on priority, with the hints previously mentioned, you'll need to create your own ground truth (target labels) on priority and explain the rationale behind this. \n",
    "\n",
    "The features you can use is left open, how you model the problem is also left open. We'd say, start with something simple first.\n",
    "\n",
    "Evaluation:  We want to see for a given email that your model has never seen, how good the model is in determining the email's priority. It's recommended that you keep a portion of the dataset here to evaluate your model. Feel free to use standard metrics or create a new ones.\n",
    "\n",
    "Email if you have any questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://docs.python.org/3/library/datetime.html\n",
    "from datetime import datetime\n",
    "def to_timestamp(d):\n",
    "    ''' Given an ISO format date string, convert to unix timestamp '''\n",
    "    return datetime.timestamp(datetime.fromisoformat(d))\n",
    "\n",
    "def to_datetime(d):\n",
    "    ''' Given an ISO format date string, convert to datetime object '''\n",
    "    return datetime.fromisoformat(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\nagak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "import re, difflib\n",
    "from sklearn.neighbors import KDTree\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def strip_spaces(s):\n",
    "    return re.sub('[ ]+', ' ', s).strip()\n",
    "\n",
    "def ContentExtraction(mail, ContentSplitters):\n",
    "    '''\n",
    "    Slicing email body content than signature and disclaimers\n",
    "    '''\n",
    "    lmtr = [lmtr for lmtr in ContentSplitters if lmtr in mail.lower()]\n",
    "    if len(lmtr)>0:\n",
    "        limiter = re.search(lmtr[0], mail.lower()).start()\n",
    "        return mail[:limiter]\n",
    "    return mail\n",
    "\n",
    "def Remove_URLs(x):\n",
    "    '''\n",
    "    Removing URLs from the mail body\n",
    "    '''\n",
    "    x = word_tokenize(x)\n",
    "    x = [i for i in x if not len(re.findall(r'[\\w\\.-]+@[\\w\\.-]+',i))]\n",
    "    x = ' '.join(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def preprocess_mail(text, punctuation=False):\n",
    "    '''\n",
    "    Preprocessing the email content \n",
    "    removing punctuations except few required\n",
    "    strip spaces\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('[^A-Za-z0-9 ,.]+', '', text.replace('\\n',' ')).lower()\n",
    "    text = strip_spaces(text)\n",
    "    text = text.replace(',',' , ').replace('.',' . ')\n",
    "    text = ContentExtraction(text, ContentSplitters)\n",
    "    text = Remove_URLs(text)\n",
    "    if punctuation:\n",
    "        text = re.sub('[^A-Za-z0-9 ]+', '', text.replace('\\n',' ')).lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "ContentSplitters  = ['best regards', 'rgds ','b rgds', '\\ngreetings', '\\nthanks.', '\\nthanks,', '\\nthank you','\\nthank you,', '\\nthank you\\n', 'sincerely', 'regard ',\n",
    "                      'regards', 'kind regards', 'the information contained in this','forwarded', '\\ntel:', '\\nMobile:', '\\nall the best,','\\ncordially',\n",
    "                      '[image: image.png]','thx','Tel:','Fax:','greeting', '\\nproject manager ', 'from:', 'envoyé :', \n",
    "                      'the information contained in this email are confid', '------- forwarded message -----', \n",
    "                      'proprietary and confidential.','\\nthanks a lot', 'tel. ','Please consider your environmental responsibility',\n",
    "                      'The administrator of your personal data','the information transmitted in this e-mai',  'Deze email en de bijgevoegde', 'this e-mail is intended only for the person or entity']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building a trie to add and remove words faster way\n",
    "class Trie:\n",
    "    head = {}\n",
    "    \n",
    "    def add(self,word):\n",
    "\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                cur[ch] = {}\n",
    "            cur = cur[ch]\n",
    "        cur['*'] = True\n",
    "\n",
    "    def search(self,word):\n",
    "        cur = self.head\n",
    "        for ch in word:\n",
    "            if ch not in cur:\n",
    "                return False\n",
    "            cur = cur[ch]\n",
    "\n",
    "        if '*' in cur:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def printf(self):\n",
    "        print (self.head)\n",
    "\n",
    "# %%timeit\n",
    "# trie_dict.search(\"paper\")\n",
    "# 503 ns ± 1.57 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
    "# %%timeit\n",
    "# 'paper' in words1\n",
    "# 1.26 ms ± 76.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
    "\n",
    "\n",
    "# Tried with recursive manner of linking mails\n",
    "# df = df.sort_values('time')\n",
    "# df = df[~df[['uniqueID','uniqueID_rsp']].duplicated()]\n",
    "# seq = []\n",
    "# for ind, row in df.iterrows():\n",
    "#     li = [[row['uniqueID'], row['time'].timestamp()], [row['uniqueID_rsp'], row['time_rsp'].timestamp()]]\n",
    "#     seq.append(sorted(li, key=lambda x: x[1]))\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "def get_nearest_word(search_w, stopwords, words1, excess=1000):\n",
    "    '''\n",
    "    Function to search nearest possible word, like \n",
    "    c[icking -> clicking\n",
    "    corrections to mail words mentioned in mail body\n",
    "    \n",
    "    '''\n",
    "\n",
    "#     words1 = [list(map(ord, i)) for i in set(words)]\n",
    "#     max_length = max(map(len, words1))\n",
    "#     words1 = pad_sequences(words1, max_length)\n",
    "#     tree = KDTree(words1, leaf_size=8)\n",
    "    if search_w in stopwords:\n",
    "        return [search_w, 1]\n",
    "    dist = 0\n",
    "    left = 0\n",
    "    right = len(words1)\n",
    "    while left<=right:\n",
    "        mid = (left+right)//2\n",
    "        d = difflib.SequenceMatcher(None, search_w ,words1[mid]).ratio()\n",
    "        if d==1:\n",
    "            break\n",
    "        elif d<dist:\n",
    "            left =  mid + 1\n",
    "        else:\n",
    "            dist = d\n",
    "            right = mid - 1\n",
    "    \n",
    "    dist = ['',0, 0]\n",
    "    for ind, w in enumerate(words1[mid-excess:mid+excess]):\n",
    "        d = difflib.SequenceMatcher(None, search_w, w).ratio()\n",
    "        if d==1:\n",
    "            dist = [w, d]\n",
    "            return dist\n",
    "        if d>dist[1]:\n",
    "            dist = [w, d]\n",
    "    return dist\n",
    "\n",
    "def concatenate_words(s, i, stopwords, words1, excess, word_size=9):\n",
    "    '''\n",
    "    search and concatenating the space delimited words \n",
    "    fell ow  -> fellow\n",
    "    coordin ate  -> coordinate\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    te = []\n",
    "    for j in [0,-1]:\n",
    "        search_w = []\n",
    "        if j==-1 and i>0:\n",
    "            search_w.append(s[i-1])\n",
    "        threshold = 0\n",
    "        for ind, w in enumerate(s[i:]):\n",
    "            \n",
    "            if threshold>word_size: break\n",
    "            threshold+=len(w)\n",
    "            search_w.append(w)\n",
    "#             print(search_w)\n",
    "#             print(''.join(search_w) in words1)\n",
    "#             r = get_nearest_word(''.join(search_w), stopwords, words1, excess=excess)\n",
    "            \n",
    "#             if r[1]==1:\n",
    "#                 return ind+1, r[0], j\n",
    "#             te.append(r)\n",
    "#             if ''.join(search_w) in words1: #r[1]==1:\n",
    "            if trie_dict.search(''.join(search_w)):\n",
    "                return ind+1, ''.join(search_w), j\n",
    "            \n",
    "    return None, '', None\n",
    "\n",
    "\n",
    "\n",
    "def sequence_processing(s, stopwords, words1, excess, word_size=6):\n",
    "    '''\n",
    "    sequence processing from to add space delimited words in sequnce\n",
    "    \n",
    "    sequence :\n",
    "    'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on \n",
    "    international issues of critical significance .'\n",
    "    \n",
    "    Out:\n",
    "    'wion is uniquely positioned as the global of india , presenting its own perspective on international\n",
    "    issues of critical significance .'\n",
    "    \n",
    "    '''\n",
    "    res = []\n",
    "    w = 0\n",
    "    while w<len(s):\n",
    "        # d = get_nearest_word(s[w], stopwords, words1, excess=excess)\n",
    "        if trie_dict.search(s[w]):\n",
    "            res.append(s[w])\n",
    "        else:\n",
    "            d1 = concatenate_words(s, w, stopwords, words1, excess, word_size)\n",
    "            if d1[2]==-1:\n",
    "                res.pop()\n",
    "                res.append(d1[1])\n",
    "            elif d1[2]==0:\n",
    "                res.append(d1[1])\n",
    "                w += d1[0]\n",
    "            else:\n",
    "                res.append(s[w])\n",
    "        w +=1\n",
    "    return ' '.join(res)\n",
    "\n",
    "\n",
    "# Building a trie of english available words\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Selecting vocal library of english words \n",
    "words = nltk.corpus.brown.words()\n",
    "words1 = []\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    w = re.sub('[0-9]','', w)\n",
    "    words1.append(w)\n",
    "words1 = list(set(words1))\n",
    "words1.sort()\n",
    "\n",
    "# adding all possible words to trie to make ease of search\n",
    "trie_dict = Trie()\n",
    "for word in words1:\n",
    "    if '*' not in word and word not in list(string.ascii_lowercase+string.ascii_uppercase+string.digits):\n",
    "        trie_dict.add(word)\n",
    "\n",
    "\n",
    "# %%time\n",
    "# s = 'wion is un iquely positioned as the globa l voice of ind ia , present ing its own perspective on international issues of critical significance .'\n",
    "# sequence_processing(s.split(' '), stopwords, words1, tune)\n",
    "# Out: 'wion is uniquely positioned as the global of india , presenting its own perspective on international issues of critical significance .'\n",
    "# before code optimization\n",
    "# Wall time: 8.43 s\n",
    "\n",
    "# after code optimization\n",
    "# 38.2 µs ± 893 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
    "\n",
    "\n",
    "# Exception need to be handled\n",
    "# s = 'if you are interested and available to talk to us through a phone call at 1030 am friday or saturd ay this w e e k , or sometime betwee n 930 and 1030 am next mond ay , please let us know .'\n",
    "# sequence_processing('this w e e k , or'.split(' '), stopwords, words1, lr, word_size=15)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "email_chain_pairs = pickle.load(open('email_chains.pkl','rb'))\n",
    "\n",
    "\n",
    "ec = email_chain_pairs\n",
    "ml = pd.DataFrame([i[0] for i in ec])\n",
    "rs = pd.DataFrame([i[1] for i in ec])\n",
    "\n",
    "ml['time']= pd.to_datetime(ml['time'], utc=True)\n",
    "rs['time'] = pd.to_datetime(rs['time'], utc=True)\n",
    "df = ml.join(rs.add_suffix('_rsp'))\n",
    "df['target'] = df['time_rsp'] - df['time']\n",
    "\n",
    "df['uniqueID'] = df[['sender', 'time', 'subject']].astype(str).apply(' - '.join,1)\n",
    "df['uniqueID_rsp'] = df[['sender_rsp', 'time_rsp', 'subject_rsp']].astype(str).apply(' - '.join,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['mail'] = np.where((df['time_rsp']-df['time'])<pd.to_timedelta('1s'), df['body_rsp'], df['body'])\n",
    "df['text'] =df['mail'].apply(preprocess_mail)\n",
    "df['target'] = df['time_rsp'].subtract(df['time']).dt.total_seconds().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i was a post doctoral fellow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'i was a post doctoral fe llow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there '\n",
    "sequence_processing(s.split(' '), stopwords, words1, excess=30000, word_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function intended map loop chain of mails recieved and forwarded from respective id in recursive manner\n",
    "\n",
    "'''\n",
    "# def recur(sq, key, res):\n",
    "#     print(key)\n",
    "#     for s in range(len(sq)):\n",
    "#         print(sq[s][2])\n",
    "#         if sq[s][2]==0 and sq[s][0][0]==key:\n",
    "#             print('got in')\n",
    "#             sq[s][2] = 1\n",
    "#             sq, res = recur(sq, sq[s][1][0], res+[sq[s][0][0]])\n",
    "#     return sq, res\n",
    "        \n",
    "#       results = []\n",
    "# for i in range(len(seq))[:3]:\n",
    "#     if key == seq[i][1][0]:\n",
    "#         print('found')\n",
    "#     if seq[i][2]==0:\n",
    "#         seq[i][2]=1\n",
    "#         seq, res = recur(seq, seq[i][1][0], [seq[i][1][0]])\n",
    "#         results.append(res)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # x = ml.sample(1).iloc[0]\n",
    "# x = df.sample(1).iloc[0]\n",
    "\n",
    "# sents = []\n",
    "# lr=4000\n",
    "# for s in nltk.tokenize.sent_tokenize(x['text']):\n",
    "#     sent = sequence_processing(s.split(' '), stopwords, words1, lr, word_size=15)\n",
    "#     sents.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['text1'] = df.apply(lambda x: ''.join(sequence_processing(sent.split(' '), stopwords, words1, excess=30000, word_size=15) for sent in nltk.tokenize.sent_tokenize(x['text'])),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr . fauci , how are you sir long ago , in the early 90s , i was a post doctoral fe llow in the laboratory of viral and molecular pathogenesis at nih . i dont know if the same lab is still there . now i work on brain injury and alzheimers but my interest in viruses and mechanisms of viral pathogenesis has not waned and the recent covid19 outbreak prompted me to do a little investigation on my own . my studies of blood microrna changes after tbi and ad suggest that principal component analysis of distinct changes in circulating mirnas can identify the patient population . microrna alterations can be measured by realtime pcrwhich i presume is the basis of the test that is developed for this disease but i am analyzing blood mi rnaseq expression profiles and now it is possible to quickly sequence blood samples in a few hours and get accurate results . blood gene expression in my studies was more var iable lots of rnasesin blood so i found that micrornas are much more stable in blood and serum samples . i attach an example of a pcahierarchical clustering heatmap analysis of a geo dataset for merscov from 2016 https www . ncbi . n lm . nih . govgeoqueryacc . cgiaccgse8l852 i performed the pca and heatmap analyses at two differen t stringencies and you can see that the patients can be unequivocably distinguished from the controls at very significant p and fdrvalues . just a thought but many clinical centers , hospitals , academic institutions can quickly perfo rm transcriptomewide sequencing . blood rna can be isolated in 12 hrs , sequencing libraries made in a few hres and one mi rna sequencing run can handle up to 48 samples and the data can be quickly analyzed . just my two cents on how nih could accelerate the analysis of new blood samples for this new strain of coronanvirus . you could mobilize hundreds of sequencing centers to help in the analysis .\n"
     ]
    }
   ],
   "source": [
    "x = df.sample(1).iloc[0]\n",
    "print(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.fauci , how are you sir long ago , in the early 90s , i was a post doctoral fellow in the laboratory of viral and molecular pathogenesis at nih .i dont know if the same lab is still there .now i work on brain injury and alzheimers but my interest in viruses and mechanisms of viral pathogenesis has not waned and the recent covid19 outbreak prompted me to do a little investigation on my own .my studies of blood microrna changes after tbi and ad suggest that principal component analysis of distinct changes in circulating mirnas can identify the patient population .microrna alterations can be measured by realtime pcrwhich i presume is the basis of the test that is developed for this disease but i am analyzing blood mi rnaseq expression profiles and now it is possible to quickly sequence blood samples in a few hours and get accurate results .blood gene expression in my studies was more variable of rnasesin blood so i found that micrornas are much more stable in blood and serum samples .i attach an example of a pcahierarchical clustering heatmap analysis of a geo dataset for merscov from 2016 https www .ncbi .n lm .nih .govgeoqueryacc .cgiaccgse8l852 i performed the pca and heatmap analyses at two different and you can see that the patients can be unequivocably distinguished from the controls at very significant p and fdrvalues .just a thought but many clinical centers , hospitals , academic institutions can quickly perform sequencing .blood rna can be isolated in 12 hrs , sequencing libraries made in a few hres and one mi rna sequencing run can handle up to 48 samples and the data can be quickly analyzed .just my two cents on how nih could accelerate the analysis of new blood samples for this new strain of coronanvirus .you could mobilize hundreds of sequencing centers to help in the analysis .\n"
     ]
    }
   ],
   "source": [
    "print(x['text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "def preprocesse_text(df, test_size=0.2, stopwords=[]):\n",
    "    '''\n",
    "    Building a N-gram based model with words sequence from 1 to 6 words \n",
    "    restring max features to 45000\n",
    "    added character based features \n",
    "    \n",
    "    concatenated both feature vectors in modelling\n",
    "    \n",
    "    '''\n",
    "    train_text,test_text,y_train,y_test = train_test_split(df['text1'], df['target'], test_size=test_size)\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(1, 6),\n",
    "        max_features=45000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words=stopwords,\n",
    "        ngram_range=(5, 6),\n",
    "        max_features=10000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    return train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plotting the model predictions with percent of change from the actual\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer = preprocesse_text(df, test_size=0.2, stopwords=nltk.corpus.stopwords.words('english'))\n",
    "reg = LinearRegression()\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     55000.000000\n",
       "mean      17871.328498\n",
       "std       35454.929593\n",
       "min           0.000000\n",
       "25%        2016.766391\n",
       "50%        5699.842152\n",
       "75%       16827.573039\n",
       "max      909134.617663\n",
       "Name: wgt, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Feature weights analyzation to either keep or discard from modelling\n",
    "'''\n",
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()\n",
    "features['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['would possible',\n",
       " 'hit chris',\n",
       " ' sick',\n",
       " 'health care system novel coronavirus outbreak',\n",
       " 'epidemic administered exposed people',\n",
       " 'spokewith',\n",
       " 'emergency use',\n",
       " 'ph chief scientific officer president',\n",
       " 'tedros tweet stephanie nebehay stephanie nebehay',\n",
       " 'services alex',\n",
       " 'themes',\n",
       " 'th warning intended email recipient',\n",
       " 'confir',\n",
       " 'moving forward gratifying speed commitment',\n",
       " 'covid19 effortsin china zy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chekcking model weights which are  most prominant in modelling\n",
    "thr = features['wgt'].quantile(0.5)\n",
    "features[features['wgt'].gt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['developing infected covid19',\n",
       " 'group idea future',\n",
       " 'tered',\n",
       " ' devi',\n",
       " 'even phase',\n",
       " 'ent c',\n",
       " 'leaders',\n",
       " ' a few',\n",
       " 'g as ',\n",
       " 'wnch111e session oncov 19 tue 310',\n",
       " 'wnch111e session oncov 19 tue',\n",
       " 'family',\n",
       " 'wnch111e session oncov 19',\n",
       " 'n for',\n",
       " 'n for ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chekcking model weights which are less contributing to model \n",
    "features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['gram_type'] = ['ngrams']*train_char_features.shape[1] + ['char grams']*train_word_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gram_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>char grams</th>\n",
       "      <td>35000.0</td>\n",
       "      <td>17148.913012</td>\n",
       "      <td>35917.197102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2206.358632</td>\n",
       "      <td>5469.385079</td>\n",
       "      <td>15448.952101</td>\n",
       "      <td>770848.134727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngrams</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>45540.138586</td>\n",
       "      <td>46540.757077</td>\n",
       "      <td>3.217177</td>\n",
       "      <td>14884.715923</td>\n",
       "      <td>33085.178872</td>\n",
       "      <td>58589.767678</td>\n",
       "      <td>601834.998813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count          mean           std       min           25%  \\\n",
       "gram_type                                                                 \n",
       "char grams  35000.0  17148.913012  35917.197102  0.000000   2206.358632   \n",
       "ngrams      10000.0  45540.138586  46540.757077  3.217177  14884.715923   \n",
       "\n",
       "                     50%           75%            max  \n",
       "gram_type                                              \n",
       "char grams   5469.385079  15448.952101  770848.134727  \n",
       "ngrams      33085.178872  58589.767678  601834.998813  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking the model weights in consideration to character and n gram based feature words\n",
    "'''\n",
    "features.groupby('gram_type')['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1852.0</td>\n",
       "      <td>45070.243919</td>\n",
       "      <td>44759.903113</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>15074.730448</td>\n",
       "      <td>33409.543852</td>\n",
       "      <td>58055.357892</td>\n",
       "      <td>403086.447567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2164.0</td>\n",
       "      <td>44831.625747</td>\n",
       "      <td>46530.124360</td>\n",
       "      <td>3.217177</td>\n",
       "      <td>14855.636205</td>\n",
       "      <td>32982.987178</td>\n",
       "      <td>58099.061334</td>\n",
       "      <td>562273.234026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1637.0</td>\n",
       "      <td>46333.091109</td>\n",
       "      <td>46556.901531</td>\n",
       "      <td>8.397256</td>\n",
       "      <td>15093.396542</td>\n",
       "      <td>33854.692454</td>\n",
       "      <td>60146.969911</td>\n",
       "      <td>394521.470373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507.0</td>\n",
       "      <td>46819.811846</td>\n",
       "      <td>50160.121468</td>\n",
       "      <td>89.464215</td>\n",
       "      <td>14728.302399</td>\n",
       "      <td>32641.000031</td>\n",
       "      <td>60193.119923</td>\n",
       "      <td>601834.998813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1440.0</td>\n",
       "      <td>45999.621937</td>\n",
       "      <td>46098.247212</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>15059.111812</td>\n",
       "      <td>33362.165177</td>\n",
       "      <td>58715.885888</td>\n",
       "      <td>428129.550783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1400.0</td>\n",
       "      <td>44479.624158</td>\n",
       "      <td>45267.767879</td>\n",
       "      <td>3.238693</td>\n",
       "      <td>14416.613255</td>\n",
       "      <td>32699.576500</td>\n",
       "      <td>56231.869112</td>\n",
       "      <td>286769.553239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count          mean           std        min           25%  \\\n",
       "features                                                                \n",
       "1         1852.0  45070.243919  44759.903113   3.238693  15074.730448   \n",
       "2         2164.0  44831.625747  46530.124360   3.217177  14855.636205   \n",
       "3         1637.0  46333.091109  46556.901531   8.397256  15093.396542   \n",
       "4         1507.0  46819.811846  50160.121468  89.464215  14728.302399   \n",
       "5         1440.0  45999.621937  46098.247212   3.238693  15059.111812   \n",
       "6         1400.0  44479.624158  45267.767879   3.238693  14416.613255   \n",
       "\n",
       "                   50%           75%            max  \n",
       "features                                             \n",
       "1         33409.543852  58055.357892  403086.447567  \n",
       "2         32982.987178  58099.061334  562273.234026  \n",
       "3         33854.692454  60146.969911  394521.470373  \n",
       "4         32641.000031  60193.119923  601834.998813  \n",
       "5         33362.165177  58715.885888  428129.550783  \n",
       "6         32699.576500  56231.869112  286769.553239  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Checking the most prominant n-gram size for modelling\n",
    "'''\n",
    "features[:train_char_features.shape[1]].groupby(features['features'].str.split(' ').apply(len))['wgt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Considering the analysis of model weights and removing unsupportive key words which are not contributing to model\n",
    "'''\n",
    "extra_stopwords = features[features['wgt'].lt(thr)].sort_values('wgt', ascending=False)['features'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['0', '00', '000', '01', '02', '02139', '04212020', '050ga', '06', '0614g', '07', '0700', '0800', '0jjnx0y2zue', '1', '10', '10003', '100125', '1002', '1007', '1019strategicp', '103', '1046', '106a', '11', '110', '11012020', '1112', '113', '1130', '1140amlpm', '119', '1194faa', '12', '1200', '12026597931', '1211', '1212402', '123', '1250', '1299', '12pm', '13', '1300', '130pm', '138', '13th', '14', '142', '1445', '14day', '15', '1500', '15minvte', '15pm', '1615', '1616', '162', '1640', '16504793208', '17', '177', '18', '19', '19104', '1918', '1927955', '197', '1970s', '1980', '1983', '1984', '1990', '19oqc', '19treatmentguidelines', '1his', '1mj', '1nfecuouso1seases', '1pm', '1t', '1who', '2', '200', '20004', '2001', '2003', '2004', '2008', '2009', '2017', '2019', '2019nco', '2019ncov', '2020', '20200303', '20201', '2023348387', '2024', '2028955060', '2031399059', '2035', '203699561', '207', '2089', '208922520', '212', '2123013100', '2123013200', '2126641289', '216', '22', '223', '224', '22hlenas', '23', '230', '24', '240', '2417', '25', '251028', '2520', '25m', '25minute', '26', '27', '28', '28029', '2802926', '2803512', '2964803', '2day', '2dwrong', '2pm', '3', '30', '300', '301', '3011983', '3014964409', '31', '310', '324', '325', '33', '34', '35am', '37', '4', '40', '400', '4001', '401', '404', '412', '41227912490', '4140521', '430', '44', '459', '48', '4805291', '4964409', '5', '50', '5001368', '5381', '55', '550', '5568314', '58', '5th', '6', '60', '615', '619h', '6273466', '633', '6393286', '6467542000', '649', '6645222', '67374', '693', '6f27', '6j', '6pm', '7', '700', '705', '710', '710amish', '730', '730am', '750', '751', '7874', '78bt3rz2mq', '7a03', '7a17c', '7a17e', '7a17f', '7am', '7hs', '8', '800', '8003695815', '8003696719', '811', '832', '84', '840am', '888', '8am', '8d34', '8th', '9', '907', '9086', '930am', '978', '9806', '99', '9910', '9a56', 'a03', 'ab', 'abbvie', 'abc', 'ability', 'abl', 'able', 'abo', 'aboard', 'abou', 'abovementioned', 'abroad', 'absence', 'absolutely', 'absorb', 'abutaleb', 'ac', 'academia', 'academies', 'academjc', 'academy', 'acc', 'acce', 'accelerate', 'accepted', 'accompanied', 'accompanying', 'according', 'accountability', 'accounts', 'accrue', 'accuracy', 'accuracyveracity', 'achieved', 'ack', 'across', 'act', 'acti', 'actions', 'activactivities', 'activate', 'active', 'actively', 'activities', 'activity', 'actual', 'actually', 'acut', 'ad', 'adam', 'adapted', 'added', 'addi', 'adding', 'addis', 'addit', 'additional', 'address', 'addressed', 'addressee', 'addresses', 'adequate', 'adequately', 'aders', 'adhanom', 'adm', 'admi', 'admin', 'administrations', 'administrator', 'adolescent', 'adva', 'advan', 'advance', 'advanced', 'advancement', 'advances', 'advantage', 'advi', 'advisable', 'advisor', 'advisors', 'advisory', 'aerosolization', 'aerosolized', 'aff', 'affairs', 'affiliated', 'africa', 'afte', 'afternoon', 'afternoonevening', 'agai', 'agciqcsl', 'age', 'aged', 'agen', 'agence', 'agencies', 'agency', 'agenda', 'agent', 'agents', 'aggregation', 'aggressive', 'ago', 'agree', 'agreed', 'agreeing', 'agreement', 'ahead', 'ahmed', 'ai', 'aids', 'ail', 'aim', 'aiming', 'airb', 'airborne', 'airi', 'airport', 'aj', 'ajlergyandinfectiousdiseases', 'ake', 'akin', 'al', 'alannpanic', 'albert', 'alert', 'alex', 'alexand', 'alexandra', 'alfred', 'algorithms', 'aliceparkny', 'alive', 'allergy', 'allergyand', 'allo', 'allowed', 'allowing', 'allows', 'allthe', 'ally', 'almagro', 'along', 'alr', 'alre', 'alrea', 'already', 'als', 'alternifolia', 'alth', 'although', 'alwa', 'alway', 'always', 'ama', 'amanda', 'amazon', 'amelie', 'america', 'american', 'americans', 'americas', 'among', 'amoun', 'amount', 'amoy', 'ampcovid', 'ample', 'amroyan', 'anal', 'analysis', 'analyst', 'ance', 'anchoring', 'ando', 'andor', 'ands', 'ange', 'animal', 'ank', 'anks', 'announce', 'announced', 'announcements', 'ano', 'another', 'anovaccines', 'answered', 'answering', 'ant', 'anthony', 'anti', 'antibody', 'antimalarial', 'antimicrobial', 'antiv', 'antivi', 'antivira', 'antiviral', 'antivirals', 'antoniak', 'anyo', 'anyone', 'anything', 'anytime', 'anywhere', 'ap', 'ap91', 'apart', 'apco', 'apcowortdwide', 'apologies', 'apologize', 'apology', 'apoptotic', 'apparent', 'appears', 'applicable', 'application', 'applied', 'appointed', 'appointees', 'appointments', 'appr', 'appreciate', 'appreciated', 'appreciateyour', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriated', 'approval', 'approx', 'approximates', 'appy', 'april', 'ar', 'arbs', 'arch', 'arcp', 'ard', 'ardal', 'ards', 'area', 'ared', 'arge', 'arisen', 'arose', 'around', 'aroundpost', 'arranged', 'arranging', 'arrival', 'arrive', 'arter', 'article', 'articles', 'artisan', 'artists', 'ary', 'aryankalayi', 'aryankalayil', 'ascasessurge', 'ase', 'ased', 'ases', 'asha', 'ashley', 'asia', 'ask', 'asked', 'asking', 'aspen', 'aspeninstin1te', 'asprs', 'ass', 'assaysand', 'assbackwards', 'assesscould', 'assessed', 'assets', 'assi', 'assis', 'assistant', 'asso', 'assoc', 'associate', 'associatedirector', 'association', 'assume', 'assumea', 'astho', 'asymptomatic', 'ate', 'ated', 'ateg', 'athletic', 'athletics', 'athome', 'atlanta', 'atrial', 'att', 'atta', 'attached', 'attachment', 'attachments', 'atte', 'attendance', 'attended', 'attendees', 'attending', 'attention', 'attorney', 'attract', 'auci', 'audi', 'audience', 'audiences', 'audio', 'augensteinwtop', 'ause', 'auth', 'authorised', 'authoritative', 'authorities', 'authority', 'authors', 'automat1cally', 'automates', 'ava', 'avai', 'availability', 'available', 'avenue', 'avery', 'avi', 'avian', 'avoid', 'award', 'awarded', 'awardees', 'aware', 'away', 'awhile', 'awhilebest', 'ay', 'aylward', 'azar', 'b', 'b4', 'b5', 'b6', 'b6cell', 'b6for', 'ba', 'baby', 'bac6674f', 'back', 'back8', 'background', 'backon', 'backup', 'bad', 'baier', 'baldwin', 'baltimore', 'bandwidth', 'barda', 'bare', 'barrier', 'barry', 'bas', 'base', 'basic', 'battifoglia', 'bauchner', 'bcast', 'bcgvaccine', 'bcun', 'bea', 'bec', 'beca', 'become', 'bed', 'bee', 'befo', 'befor', 'beg', 'begi', 'begin', 'beha', 'behal', 'behalf', 'behalfofthe', 'behind', 'beige', 'beijing', 'bein', 'believe', 'believed', 'bellevue', 'benefits', 'bernie', 'bes', 'besides', 'best', 'bestowed', 'beta', 'bethesda', 'beyond', 'bg', 'bh6', 'bi', 'bid', 'big', 'biggest', 'bile', 'bill', 'billet', 'bin', 'binding', 'bio', 'bioavailability', 'bioch', 'biochemist', 'biology', 'biontech', 'biorxiv', 'bioscie', 'biosecurity', 'biotech', 'biotechs', 'birx', 'blackpepperplus', 'bless', 'block', 'blood', 'blue', 'bo', 'boar', 'bod', 'body', 'boghosian', 'bokvjsyqvb', 'booking', 'bookings', 'boone', 'boonefoxnews', 'boston', 'botched', 'bounce', 'box', 'brain', 'brainstorm', 'branch', 'brand', 'brandon', 'braves', 'breakfast', 'breaking', 'breakthroughs', 'brenatecourtney', 'brennan', 'brian', 'bridge', 'brie', 'briefed', 'brightest', 'bringing', 'brittany', 'broad', 'broadly', 'broadway', 'brooks', 'bruce', 'brundtlandand', 'bs', 'bsl3samples', 'bttpsscbolar', 'bu', 'built', 'bulletin', 'bureau', 'burn', 'business', 'businessstandard', 'busy', 'butterf', 'butterfield', 'buzzfeed', 'bv', 'bw7cl', 'bx5pcp', 'c', 'c1itically', 'ca', 'cable', 'caitlin', 'cal', 'california', 'call', 'called', 'callin', 'calm', 'cambridge', 'came', 'cameras', 'campaign', 'canada', 'canadian', 'cancel', 'canceled', 'cancellations', 'candidates', 'cans20200220b6', 'cant', 'capa', 'capabilities', 'capacity', 'capital', 'capitol', 'capture', 'captured', 'carbon', 'cardamom', 'cardiology', 'care', 'cared', 'careers', 'cari', 'carol', 'carolina', 'carter', 'carve', 'cas', 'case', 'cases', 'cassia', 'catalytic', 'cate', 'categ', 'categories', 'categories1', 'cationic', 'cats', 'causes', 'causing', 'cb', 'cb6', 'cbh6j', 'cbh6l', 'cbh6poznansky', 'cc', 'ccept', 'ccmcds', 'cdc', 'cdcbut', 'cdcemail', 'cdcemployee', 'cdchas', 'cdchave', 'cdcon', 'cdcor', 'cdcs', 'cdcscientists', 'cdcteams', 'ce', 'celebrated', 'celebration', 'cell', 'cells', 'cen', 'cent', 'centers', 'central', 'ceoand', 'ceos', 'ceosof', 'certain', 'certainly', 'ces', 'cest', 'cetand', 'ch', 'cha', 'chaffetz', 'chai', 'chain', 'chair', 'chairs', 'chal', 'chall', 'challenge', 'challenges', 'chamber', 'championing', 'chan', 'chang', 'change', 'changes', 'changing', 'channel', 'channels', 'chanschool', 'chaos', 'chapter', 'char', 'characteristics', 'characterized', 'chastise', 'chat', 'chats', 'cheap', 'check', 'cheers', 'chemistry', 'chen', 'chf', 'chi', 'chicago', 'chief', 'children', 'chinas', 'chinese', 'chinesecenter', 'chloroquine', 'choicesperhaps', 'chosen', 'christie', 'christine', 'chronic', 'ci', 'cial', 'cieslak', 'cines', 'cing', 'circling', 'circulatingair', 'circumstances', 'cities', 'citizens', 'city', 'cjcking', 'ckly', 'cl', 'cl0', 'clafp', 'claire', 'clarify', 'clarity', 'classes', 'classic', 'clea', 'cleaner', 'clear', 'clearance', 'cleared', 'clearly', 'clem', 'cleverly', 'cli', 'clic', 'clickhere', 'cliff', 'clifford', 'clin', 'clini', 'clinic', 'clinical', 'clinically', 'clinicians', 'clinics', 'clip', 'clips', 'clo', 'clos', 'closed', 'closely', 'closings', 'closure', 'cloves', 'cm', 'cmelchor', 'cnbcbut', 'cnn', 'co11sentto', 'co80498', 'coast', 'cobtoday', 'cocacan', 'cochaired', 'code', 'coffin', 'cofounder', 'cohen', 'cohorting', 'coincidence', 'coke', 'colin', 'collaboration', 'collaborative', 'collaboratively', 'collaborators', 'colleague', 'colleagues', 'collecting', 'collection', 'collectively', 'colleen', 'college', 'collins', 'colorized', 'com', 'com11essentialspicesfor', 'comabout', 'combine', 'come', 'comedy', 'comes', 'comforting', 'comm', 'commend', 'comment', 'commenting', 'commission', 'commissioner', 'commit', 'committee', 'committees', 'common', 'communicable', 'communications', 'communities', 'community', 'comnews', 'comoutlookco', 'compagno', 'company', 'companys', 'comparison', 'compassionate', 'competence', 'competitions', 'complained', 'complementary', 'completed', 'complexities', 'complicated', 'component', 'composition', 'comprehensive', 'computers', 'comscholarhl', 'comseattlenewstimeswatchdogwhytheseattlesounders', 'comunicaci6n', 'comv2urluh', 'con', 'conc', 'concern', 'concerned', 'concerning', 'concerns', 'conclusions', 'concomitant', 'concur', 'conduct', 'conducting', 'conf', 'conference', 'conferences', 'confiden', 'confidence', 'confidential', 'confidentiality', 'confinned', 'confirm', 'confirmation', 'confirmed', 'conlrnl', 'connect', 'connection', 'conrad', 'cons', 'consent', 'consequently', 'conservation', 'conserve', 'consideration', 'considerationi', 'considerations', 'considering', 'constraints', 'consultant', 'cont', 'contact', 'contacting', 'contacts', 'contain', 'contained', 'containment', 'contains', 'content', 'content85706789', 'contents', 'context', 'continue', 'continued', 'continues', 'contract', 'contrast', 'contrasting', 'contribute', 'contributions', 'control', 'controlthe', 'conv', 'convene', 'convenient', 'convening', 'conversation', 'conversationwith', 'convinced', 'coordinated', 'copied', 'copies', 'copromot', 'copy', 'copynotes', 'copyright', 'cori', 'coriander', 'cornell', 'coro', 'coroii', 'coronav', 'coronavi', 'coronavirus', 'coronaviruscasesonboardare', 'coronaviruscruiseship', 'coronaviruses', 'coronov', 'corotl', 'corporate', 'corporations', 'corps', 'corr', 'corre', 'correo', 'correspondence', 'correspondent', 'corticosteroid', 'corticosteroidsnonsteroidalantiinflammatory', 'cost', 'costs', 'cou', 'cough', 'coul', 'could', 'coun', 'count', 'counted', 'countermea', 'countries', 'country', 'countrys', 'county', 'couple', 'coupled', 'course', 'courses', 'courtney', 'cov', 'coverage', 'covered', 'covers', 'covi', 'covi0', 'covid19', 'covid19and', 'covid19virus', 'coyid19', 'created', 'creators', 'crew', 'critical', 'croi', 'cruise', 'cste', 'cted', 'ctly', 'ctor', 'ctsof', 'cu32', 'cumin', 'cur', 'curable', 'curcumin', 'cure', 'cures', 'curevac', 'curi', 'curio', 'curious', 'curr', 'curre', 'current', 'currentaffairs', 'currently', 'curry', 'curtailing', 'curtain', 'curve', 'cus', 'cuss', 'customer', 'cute', 'cyndi', 'cynthia', 'd19', 'darikradio', 'darin', 'date', 'dates', 'datos', 'davids', 'dayhour', 'daylight', 'days', 'ddwif', 'dear', 'deaths', 'debated', 'debbi', 'declared', 'decline', 'deco', 'decompress', 'deeply', 'deeptech', 'default', 'defeat', 'defect', 'defend', 'deficit', 'defined', 'definition', 'degree', 'degrees', 'dela', 'delays', 'dele', 'delet', 'delicateness', 'deliver', 'dem', 'dema', 'demo', 'demonstration', 'denying', 'depa', 'depar', 'department', 'ders', 'desaturatio', 'described', 'descriptions', 'design', 'designated', 'destination', 'destroying', 'deta', 'detailed', 'details', 'dete', 'determine', 'dev', 'deve', 'develop', 'developed', 'developsnih', 'devi', 'devices', 'devil', 'dge', 'di', 'dia', 'diab', 'diagn', 'diagnosis', 'dialin', 'diamond', 'dies', 'diff', 'different', 'difficulties', 'diffusion', 'ding', 'dinner', 'dioxide', 'diplomatic', 'dir', 'dire', 'direction', 'director', 'directorgeneral', 'directors', 'disc', 'disclaimer', 'discussed', 'dise', 'diseaes', 'diseases', 'diseasesniaid', 'dist', 'distance', 'distancing', 'distilled', 'distribute', 'distribution', 'divi', 'diviner', 'division', 'divisions', 'dling', 'doe', 'doepel', 'doerr', 'doesnt', 'doin', 'domestic', 'donald', 'dong', 'donna', 'double', 'doubt', 'dozen', 'draf', 'draft', 'dramatically', 'dream', 'dren', 'dri', 'drinking', 'driven', 'driver', 'drs', 'drumbeat', 'ds', 'dying', 'e', 'earlier', 'earliest', 'earlymid', 'eas', 'east', 'eastern', 'easy', 'eat', 'ebex', 'ebol', 'ebolalike', 'eck', 'eclectic', 'economic', 'edge', 'edic', 'edit', 'edited', 'editing', 'edition', 'eds', 'edu', 'education', 'educational', 'eed', 'eeds', 'eeks', 'een', 'ef', 'eff', 'effe', 'effec', 'effectiveness', 'effects', 'effo', 'efforts', 'effortsto', 'egic', 'egy', 'eing', 'einstein', 'eir', 'eiscourse', 'eisinger', 'eith', 'eithe', 'either', 'eitics', 'elected', 'electron', 'elevate', 'ell', 'elp', 'ely', 'emai', 'email', 'emailutm', 'embed', 'emer', 'emerg', 'emergence', 'emergency', 'emerging', 'emily', 'emistry', 'empathy', 'employee', 'ems', 'en', 'enables', 'ence', 'endemic', 'endocrinology', 'endorse', 'enduring', 'enforced', 'engagement', 'england', 'english', 'enjoy', 'enlisting', 'eno', 'enou', 'enoug', 'enroll', 'ensure', 'ent', 'entering', 'ently', 'entry', 'ents', 'environment', 'eoc', 'epicenter', 'epicentre', 'epidemic', 'epidemiologist', 'eral', 'ered', 'ergo', 'ergy', 'ericmstrauss', 'erika', 'ern', 'ernet', 'err', 'erro', 'erts', 'erve', 'ery', 'es', 'ese', 'esnt', 'espe', 'especially', 'ess', 'ests', 'et', 'eta', 'etary', 'etc', 'ete', 'etes', 'etty', 'europe', 'europe44', 'ev', 'eva', 'evaluate', 'evaluating', 'evaluation', 'even', 'events', 'eventually', 'ever', 'every', 'excellent', 'exciting', 'excuse', 'exec', 'execute', 'exit', 'exp', 'expa', 'expansion', 'expe', 'expedited', 'expenses', 'expertise', 'explore', 'explored', 'expr', 'expre', 'express', 'expressed', 'extend', 'external', 'extinctionand', 'extr', 'ey', 'ezekiel', 'facc', 'facebook', 'faced', 'facilities', 'facing', 'fact', 'failed', 'fair', 'fake', 'falcons', 'fall', 'fam', 'fami', 'faster', 'fau', 'fauc', 'fauti', 'fautt', 'fda', 'fe', 'fea', 'fear', 'fears', 'feasible', 'featured', 'features', 'feb', 'fecal', 'fect', 'fellow', 'fellowship', 'fewer', 'ff', 'fhe', 'fi', 'fice', 'field', 'figure', 'figures', 'fin', 'finally', 'finance', 'financial', 'find', 'finds', 'firing', 'firs', 'first', 'fit', 'fl', 'flints', 'fo', 'focu', 'focused', 'fol', 'folkers', 'folks', 'foll', 'food', 'forces', 'fore', 'foreign', 'form', 'formally', 'formerly', 'forms', 'forum', 'found', 'foundation', 'fr', 'francepresse', 'friday', 'front', 'fta9', 'fullpdf', 'funded', 'fundingsupport', 'furneri', 'furth', 'future', 'fw', 'g', 'gabbard', 'gabrielle', 'gabrielyan', 'gao', 'gaps', 'garden', 'gardens', 'gates', 'gave', 'gavi', 'ge', 'gelfand', 'gency', 'gene', 'generally', 'generosity', 'genetic', 'geneva', 'gent', 'genuine', 'georgetown', 'ges', 'gets', 'getting', 'gh', 'ght', 'ghts', 'ging', 'giroir', 'giv', 'giving', 'goes', 'goldnern', 'goo', 'google', 'gostin', 'gove', 'government', 'governments', 'governor', 'gpmbcochairsdr', 'gr', 'gram', 'gre', 'grea', 'greece', 'greek', 'greenroom', 'grigsby', 'gton', 'guess', 'guesstimate', 'guid', 'guidelines', 'gulick', 'gy', 'han', 'hand', 'handle', 'hands', 'happ', 'happily', 'harasth', 'hard', 'hardly', 'harrisons', 'harvard', 'harvardt', 'hat', 'hav', 'hcb', 'hea', 'head', 'headed', 'heading', 'headquartered', 'headquarters', 'heads', 'headsup', 'heal', 'healt', 'health', 'healthhhs', 'healthy', 'hear', 'heard', 'heari', 'hearing', 'heart', 'heath', 'hel', 'help', 'helpful', 'helps', 'hemisphere', 'hemodynamic', 'henry', 'heres', 'hese', 'hfbclidiwar', 'hhs050ga', 'hi', 'hich', 'hief', 'hig', 'high', 'higher', 'highrisk', 'hile', 'hill', 'hima', 'himyou', 'hina', 'hing', 'hink', 'historical', 'history', 'hit', 'hite', 'hlenas', 'hltps', 'ho', 'hoarse', 'hoes', 'hold', 'holder', 'holidays', 'hom', 'hone', 'honor', 'hopefully', 'hort', 'hos', 'hose', 'hoshi', 'hosp', 'hospital', 'hospitalizationi', 'hospitalized', 'hospitals', 'hotels', 'hou', 'hous', 'householdca11er', 'households', 'housekeeping', 'hover', 'howard', 'however', 'hq', 'hsencp2anqtz', 'hsieh', 'htipsgrant', 'htmifjkim10', 'htn', 'https', 'httpscholar', 'httpsljwww', 'httpsnationalpress', 'httpwww', 'hubei', 'hudson', 'huge', 'huma', 'humanity', 'hynds', 'hypertension', 'hypothetical', 'hypoxia', 'i0hih', 'ia', 'iaid', 'ials', 'ian', 'ianetikanamedia', 'ians', 'iate', 'ible', 'ibly', 'ica', 'ical', 'ice', 'iceberg', 'ick', 'id1', 'id19', 'id9ozcn', 'idea', 'ideally', 'ideas', 'identify', 'identifying', 'ideology', 'ieldfoxnews', 'iencemag', 'ies', 'ieto', 'ieve', 'iews', 'igh', 'ight', 'ignorant', 'ihistution', 'ike', 'ile', 'ill', 'illinois', 'illn', 'illne', 'illness', 'illustration', 'ily', 'im', 'image', 'imagine', 'ime', 'imes', 'imit', 'imm', 'imme', 'immediate', 'immu', 'immune', 'immunebased', 'immunologyand', 'imp', 'impactful', 'impacting', 'impacts', 'implemented', 'implications', 'impo', 'impo1tant', 'importantand', 'importantly', 'imposed', 'improvement', 'inability', 'inactivated', 'inal', 'inc', 'ince', 'incl', 'includes', 'including', 'incr', 'incre', 'increase', 'increased', 'increases', 'increasingly', 'incubation', 'indebted', 'indeed', 'indi', 'indiancooking223152', 'indias', 'indicated', 'indications', 'indiv', 'individual', 'individuals', 'indonesia', 'induce', 'induced', 'industry', 'ined', 'ines', 'inf', 'infe', 'infection', 'infectious', 'infl', 'inflection', 'influencing', 'info', 'informationnih000576', 'ings', 'ingunited', 'inidbit', 'init', 'iniza', 'ink', 'innate', 'inning', 'innovations', 'inpa', 'inpatient', 'inpatients', 'input', 'ins', 'inst', 'instead', 'institute', 'instituteorg', 'institutesojheath', 'int', 'inte', 'integrated', 'integrity', 'intended', 'intends', 'interact', 'interacting', 'interactions', 'interest', 'interested', 'interesting', 'interests', 'intereststeve', 'interestwho', 'interferonbeta', 'international', 'internationally', 'internshipresidency', 'interview', 'interviews', 'introduce', 'introductions', 'inundated', 'inute', 'inve', 'investigation', 'investigations', 'invi', 'invo', 'involved', 'involves', 'invt', 'iologyof', 'ionagendo', 'ious', 'ir', 'ira', 'iral', 'irked', 'irvsesan', 'irvsesand', 'iscoverable', 'ished', 'island', 'isnt', 'isol', 'isola', 'isolate', 'isolated', 'isolation', 'israel', 'issu', 'ists', 'ital', 'italian', 'italy', 'itecoronavirusemergency', 'ited', 'ites', 'itionscorrect', 'itter', 'itunesu', 'iumemailut', 'iur3u', 'iv', 'iv3y8bicphkwb', 'ive', 'iven', 'ivermectinaffectsthromboxane', 'iveu', 'ized', 'j980s', 'jacobsen', 'jan', 'january', 'jarris', 'jay', 'ject', 'jen', 'jeremy', 'jernigan', 'jerome', 'jess', 'joanna', 'job', 'john', 'johnso', 'johnson', 'join', 'joined', 'jointly', 'jon', 'jour', 'joyce', 'jr', 'jrt', 'judgment', 'jump', 'jumping', 'juncture', 'june', 'jus', 'kabir', 'kadlec', 'kaldec', 'kaletra', 'kathleen', 'kathrin', 'kathy', 'katie', 'katz', 'ke', 'kee', 'keeping', 'kely', 'kept', 'kers', 'kevin', 'key', 'ki', 'kiemer', 'killscovid', 'kn', 'knight', 'kno', 'knockout', 'know', 'known', 'koerber', 'korea', 'kornel', 'kraft', 'kristen', 'ks', 'lab', 'lace', 'laid', 'lam', 'lampront', 'lan2020', 'land', 'landscape', 'lane', 'langer', 'langone', 'laoshi', 'lapoo', 'lar', 'largescale', 'larry', 'last', 'lasting', 'latest', 'launch', 'launched', 'laura', 'laurie', 'lavished', 'law', 'lea', 'leadershipstudio', 'leading', 'leads', 'lear', 'leaves', 'lebanon', 'lecture', 'legal', 'legally', 'legislative', 'lescen', 'lesirfaai04032', 'lespar201', 'lesson', 'let', 'lete', 'lethality', 'letter', 'lev', 'leve', 'level', 'levels', 'leverage', 'lf', 'lfwe', 'lguidepa', 'li', 'lia', 'liab', 'liability', 'liaisons', 'liar', 'lic', 'licence', 'lick', 'licy', 'life', 'lifesaving', 'light', 'lik', 'likel', 'lilelynimmunitynince', 'limit', 'limited', 'limiting', 'ling', 'link', 'linkage', 'lipkin', 'lipkins', 'lisa', 'listcdc', 'listens', 'listserv', 'lives', 'livevideo', 'living', 'liz', 'lj', 'lled', 'llle376m22', 'llow', 'lly', 'lmart', 'lnstttute', 'lobal', 'located', 'location', 'logged', 'login', 'lop', 'loped', 'lopinavirritonavir', 'louder', 'low', 'lower', 'lp', 'lrf', 'lu', 'lude', 'lunches', 'lunchllllesessiononcxmj', 'lung', 'lwar3fsecauunrssyhca6kpj9z0npamm3ba7htufppq9b1pvrf9kdwayatcy0', 'lxty9bxj0jsl7hjtdq', 'lyn', 'lyne', 'maccallum', 'machines', 'made', 'madrid', 'maeda', 'mai', 'mailbox', 'mailtoruth', 'main', 'mak', 'malaria', 'malaysia', 'malliou', 'mana', 'mandatory', 'manner', 'manufacturers', 'manufactures', 'mar', 'mark', 'marks', 'martin', 'mas', 'materials', 'mathematical', 'mats', 'max', 'may', 'maybe', 'mber', 'mbio', 'mccullough', 'mdphd', 'means', 'meanwhile', 'mechanisms', 'med', 'medi', 'media', 'medication', 'medications', 'medicine20th', 'medicines', 'medkine', 'mee', 'meet', 'meetingcall', 'melaleuca', 'melinda', 'mely', 'mem', 'memb', 'members', 'membership', 'memo', 'memory', 'menichini', 'mensaje', 'ment', 'mentioned', 'mes', 'mess', 'message', 'messageand', 'messageis', 'mi', 'mice', 'michael', 'micr', 'microbiome', 'micronutrientsfor', 'microsoft', 'middle', 'mig', 'migh', 'mil', 'mily', 'minim', 'minimal', 'minimize', 'minimizing', 'minister', 'ministers', 'minu', 'minut', 'minute', 'minutes', 'misconduct', 'mishneh', 'misinfom1ation', 'missed', 'mission', 'mit', 'mitigating', 'mitrecorporat', 'mmunization', 'mnata', 'mob', 'mobi', 'mobilized', 'mode', 'moderate', 'moderna', 'modest', 'modifiers', 'moir', 'mong', 'monitorees', 'mor', 'morning', 'mornings', 'morrison', 'mortality', 'mory', 'mos', 'mote', 'mounted', 'moves', 'moving', 'mple', 'mply', 'mportat', 'mr', 'mrnaconference', 'msc', 'msnbc', 'mtg', 'mu', 'muc', 'muchneeded', 'multiple', 'multisite', 'municipalities', 'mus', 'mutate', 'na', 'nac', 'naccho', 'nacetylcy', 'nacetylcysteine', 'nairxsktabh', 'nal', 'name', 'nary', 'nat1on', 'nate', 'nationalism', 'nations', 'natriur', 'naugensteinwtop', 'navi', 'navirus', 'nbc', 'nbcuniversal', 'nce', 'nced', 'nday', 'nded', 'nder', 'ndiagl', 'nds', 'neal', 'nebraska', 'nebulization', 'nec', 'nece', 'necessarily', 'ned', 'nee', 'need', 'needed', 'needing', 'needs', 'negatives', 'negotiations', 'nes', 'ness', 'nest', 'network', 'networks', 'new', 'news', 'next', 'nga', 'nge', 'nger', 'nges', 'ni', 'nia', 'niai', 'niaidcoy', 'niaidhih', 'nic', 'nice', 'nics', 'nih', 'nih0', 'nih000004', 'nih000186', 'nih000187', 'nih0002', 'nih000221', 'nih000477', 'nih000518', 'nih000529', 'nih00053', 'nih000530', 'nih000573', 'nih000574', 'nih000575', 'nih000636', 'nih000804', 'nih000954', 'nih000986', 'nih001037', 'nih001038', 'nih001125', 'nih001133', 'nih001231', 'nih001258', 'nih001259', 'nih001308', 'nih001411', 'nih001434', 'nih001451', 'nih001580', 'nih001582', 'nih001584', 'nih001585', 'nih001586', 'nih001609', 'nih001610', 'nih001611', 'nih001636', 'nih001663', 'nih00167', 'nih001679', 'nih001682', 'nih001796', 'nih00184', 'nih001859', 'nih001874', 'nih00188', 'nih001957', 'nih001958', 'nih002008', 'nih002078', 'nih002079', 'nih002080', 'nih002111', 'nih002246', 'nih002328', 'nihniaid', 'nihod', 'nine', 'ning', 'ninio', 'nity', 'nitzav', 'nk', 'nking', 'nks', 'nlald', 'nlhniald', 'nm', 'nonc', 'nonco', 'nonprofit', 'nonreporting', 'nonspecific', 'noparalle', 'norm', 'normally', 'north', 'northern', 'northwest', 'nosocomial', 'notably', 'note', 'noti', 'notified', 'notify', 'nov', 'nove', 'novel', 'np', 'ns', 'nse', 'nt', 'nted', 'nter', 'nters', 'nthniaid', 'ntly', 'nto', 'ntry', 'nts', 'ntumer', 'numb', 'numberof', 'numbers', 'nurse', 'nutmg', 'nutrition', 'nyc', 'nyt', 'nyu', 'nz', 'nz7fhshorkrp9f', 'oakley', 'obama', 'objection', 'obtain', 'obviously', 'occurred', 'oday', 'odm2xsozsh438n5c', 'oevelo', 'ofan', 'ofcoiid19', 'offer', 'office', 'officer', 'officers', 'offices', 'official', 'offit', 'offlabel', 'ofo', 'often', 'ohort', 'oids', 'oijt6rea', 'oint', 'oiseasecontrochina', 'ok', 'ol', 'old', 'older', 'olea', 'om', 'ome', 'oncov', 'ondemand', 'onehour', 'oniaknbcuni', 'onli', 'online', 'onnel', 'ont', 'ood', 'ook', 'oom', 'oomvpdatefromchfli', 'oon', 'op', 'opdivstaffdiv', 'opdivstaffdivcurrent', 'opdivstaffdivkey', 'ope', 'opening', 'oper', 'operator', 'operators', 'ople', 'opportunities', 'opportunity', 'optimally', 'option', 'oqtur', 'oral', 'orde1', 'ore', 'oregon', 'org', 'org10', 'orga', 'organ', 'organizations', 'organized', 'organizing', 'ori', 'orig', 'origi', 'original', 'originally', 'originator', 'orld', 'orm', 'orne', 'ors', 'orthopedics', 'orts', 'ory', 'ose', 'osed', 'ost', 'ot', 'oth', 'othe', 'ough', 'ould', 'ound', 'ount', 'ous', 'outb', 'outbr', 'outbreak', 'outcomes', 'outl', 'outlying', 'outpatients', 'outreach', 'oval', 'ove', 'ovel', 'overalmolecularaspect', 'ovid', 'oviws6', 'ovlw', 'ovt6rea', 'ow', 'oxanebtng', 'oxidase', 'p1imary', 'pacific', 'painful', 'pand', 'pane', 'panicked', 'pap', 'par', 'parade', 'parasiticdrug', 'part', 'particles', 'particular', 'particularly', 'partly', 'partners', 'partnerships', 'parts', 'pass', 'passcode', 'passengersaboardthe', 'passing', 'past', 'pat', 'patbrittenden', 'pati', 'patie', 'patience', 'patient', 'patients', 'patr', 'patri', 'patricia', 'patrick', 'patty', 'paul', 'pdates', 'pe', 'pect', 'ped', 'pedi', 'pedv', 'peep', 'peer', 'peerreviewed', 'peers', 'pen', 'pence', 'penn', 'pennsylvania', 'peo', 'peop', 'people', 'percent', 'permission', 'personal', 'personnel', 'pharma', 'philadelphia', 'philippines', 'philly', 'phlebotomist', 'phone', 'physically', 'physiology', 'phytochemical', 'pick', 'picture', 'piec', 'pierce', 'ping', 'piscatawa', 'pivot', 'pivoting', 'pla', 'plac', 'placebocontrolled', 'placed', 'plan', 'planned', 'planning', 'planproposal', 'plans', 'plaza', 'ple', 'plea', 'pleased', 'pleasetreat', 'plunging', 'pm300', 'pncfu', 'po', 'pocket', 'points', 'poli', 'polic', 'policymakers', 'polio', 'poliomyeli', 'politics', 'poll', 'port', 'portfolio', 'pos', 'posi', 'posit', 'positive', 'positively', 'poss', 'possi', 'possibility', 'post', 'posted', 'postinfectiou', 'posture', 'pote', 'poten', 'potent', 'potential', 'powerless', 'pp', 'pply', 'ppy', 'pr', 'practice', 'practiced', 'practices', 'practitioners', 'prec', 'precludes', 'preferred', 'preliminory', 'prep', 'pres', 'present', 'presented', 'presenter', 'presenting', 'preserve', 'presser', 'pressreed', 'pri', 'price', 'primaiy', 'prin', 'prio', 'prioritization', 'prioritizing', 'priv', 'private', 'privileged', 'prize', 'process', 'produce', 'producer', 'produces', 'productionand', 'products', 'profe', 'professor', 'profile', 'prog', 'prognosis', 'program', 'programs', 'progress', 'progression', 'prohibited', 'proj', 'prominent', 'promotion', 'proofioint', 'prop', 'propagated', 'proposal', 'proposed', 'proprietary', 'prostaglandin', 'prot', 'prote', 'protect', 'protecting', 'protection', 'protective', 'protocol', 'prov', 'provide', 'provides', 'province', 'provlfe', 'prx', 'pswww', 'psxovmmimlzw', 'pt', 'pu', 'pub', 'publ', 'public', 'publications', 'pulmonaryand', 'pur', 'purs', 'pursue', 'put', 'pye', 'q', 'qu', 'quality', 'quarantine', 'que', 'ques', 'questioni', 'qui', 'quote', 'quoted', 'qy', 'r', 'r473', 'ra', 'rabin', 'rabinnyt', 'radabaugh', 'radabaughfoxnews', 'radi', 'rahhal', 'rahhalmailonline', 'raised', 'ral', 'range', 'rap', 'rapi', 'ratabl', 'rates', 'rather', 'rationale', 'rbtng', 'rday', 'rded', 'rden', 'rea', 'reac', 'reaching', 'readers', 'readership', 'reading', 'readynow', 'reak', 'reality', 'realize', 'rearranged', 'reas', 'reason', 'rec', 'recall', 'rece', 'received', 'reci', 'reco', 'recognizes', 'recordedupdate', 'recording', 'recovery', 'recurrent', 'recycled', 'red', 'redfi', 'redfie', 'reelection', 'reemerging', 'ref', 'reference', 'refocus', 'rega', 'registration', 'regulatory', 'reilly', 'related', 'relations', 'relatively', 'relavant', 'relaxing', 'relay', 'release', 'relevant', 'reliable', 'reliance', 'remarkable', 'remarks', 'remdesivir', 'rep', 'replyall', 'repo', 'reportedly', 'reports', 'represent', 'representation', 'representatives', 'represents', 'reproduce', 'republican', 'requestb6', 'requesting', 'requests', 'res', 'researchers', 'resending', 'resilience', 'reso', 'resource', 'resources', 'resp', 'respond', 'responded', 'response', 'responses', 'responsibly', 'responsive', 'ress', 'restrained', 'results', 'return', 'reup', 'reverse', 'reviewed', 'reviews', 'reynolds', 'rgo', 'rgy', 'rhe', 'rhode', 'rial', 'ribution', 'righ', 'right', 'rights', 'ring', 'rio', 'rioux', 'risk', 'rive', 'rm9a3j', 'rmed', 'rnal', 'rned', 'ro', 'road', 'robert', 'robin', 'robot', 'rockville', 'rok', 'rom', 'ronavirustest', 'ror', 'rosa', 'rough', 'round', 'roundtable', 'roy', 'rrets', 'rs', 'rter', 'rtpcrto', 'rts', 'rubenstein', 'rubin', 'ruler', 'run', 'rusi', 'rve', 's1mday', 'sabrina', 'safely', 'saffron', 'sam', 'samples', 'samsungs', 'sander', 'sar', 'saric', 'sars', 'sarsc', 'sarscov2genie', 'sary', 'sasmpgglp', 'sat', 'satellite', 'save', 'say', 'says', 'scanning', 'scenarios', 'scene', 'sche', 'scholarstartog22backon', 'schoolnik', 'schools', 'sci', 'scie', 'sciences', 'scientific', 'screening', 'sd6', 'sda', 'sdt02c5g', 'sdt02c5qch1oroquinecovid', 'sebelius', 'secondary', 'secretary', 'secretion', 'sections', 'sector', 'sed', 'sedel', 'seeking', 'seem', 'seems', 'seen', 'sel', 'selgrade', 'sending', 'sens', 'sense', 'sensitive', 'sent', 'sequenced', 'ser', 'seriously', 'serologies', 'serosurveys', 'serv', 'servi', 'services', 'ses', 'sesoveral', 'sess', 'sessi', 'session', 'sf0f', 'sh', 'sha', 'shared', 'sharing', 'shealah', 'shed', 'sheet', 'shelves', 'shifting', 'sho', 'shoddy', 'shor', 'shortages', 'shortly', 'shou', 'shown', 'shows', 'sic', 'signal', 'signals', 'signoffmmwrmedia', 'sinc', 'sing', 'singapore', 'single', 'sion', 'sis', 'sit', 'site', 'situated', 'sity', 'sive', 'skypeor', 'skyrocketed', 'slides', 'slowed', 'sly', 'smalhithehi', 'snt', 'solely', 'som', 'something', 'somethingwil', 'son', 'soon', 'sooner', 'soonest', 'sophia', 'sorry', 'sort', 'sought', 'source', 'sources', 'south', 'space', 'spans', 'speaker', 'speakers', 'speaking', 'spec', 'specialists', 'specialized', 'specializesin', 'specific', 'speed', 'spice', 'spokesper', 'spokewith', 'spook', 'spre', 'spreaders', 'srivastava', 'srnalhithehill', 'ss', 'ssly', 'stable', 'staf', 'staiie', 'stakeholders', 'stan', 'standard', 'standardsthe', 'standpoint', 'stanford', 'start', 'states', 'states2020030sa6ced5aa', 'station', 'stations', 'statisticians', 'statti', 'status', 'stces', 'step', 'stephanie', 'stepping', 'stigall', 'storage', 'stored', 'str', 'stra', 'stratification', 'strauss', 'stretched', 'strict', 'strictly', 'stripping', 'strong', 'strongly', 'stud', 'studies', 'studio', 'sual', 'subsequently', 'substantive', 'sues', 'sufficient', 'sug', 'sugg', 'suggested', 'suite', 'suject', 'summary', 'summit', 'sund', 'sup', 'supe', 'supp', 'supplies', 'supporting', 'sur', 'surg', 'surgeons', 'surprised', 'surrogate', 'surrounding', 'sus', 'susce', 'susceptible', 'swab', 'swabs', 'sym', 'symbol', 'symp', 'symptomatic', 'sys', 'syst', 'syste', 'szxovvbo1shcr025fpiwgmcm3', 'tafuk', 'tages', 'tak', 'take', 'takes', 'tal', 'talent', 'talks', 'tals', 'tand', 'tara', 'task', 'tate', 'tb', 'tbd', 'techn', 'tegy', 'tel', 'tele', 'telephone', 'television', 'tely', 'tem', 'temporary', 'tenuous', 'terribly', 'territories', 'ters', 'tes', 'tested', 'testimony', 'th', 'tha', 'thai', 'thatd', 'theodora', 'theorical', 'ther', 'therap', 'therapeutic', 'therapeutics', 'therefore', 'therein', 'thes', 'thi', 'thin', 'thinking', 'third', 'thomas', 'thos', 'thought', 'thoughts', 'threats', 'three', 'thtsolitovima', 'thx', 'ti', 'tial', 'tics', 'tied', 'tify', 'tighter', 'tim', 'time', 'timelines', 'timely', 'tion', 'tive', 'tly', 'tnfalpha', 'tnihumans', 'tobias', 'today', 'told', 'tom', 'tomor', 'tool', 'toronto', 'tors', 'tory', 'total', 'touch', 'towa', 'town', 'trade', 'transcription', 'transcripts', 'transmissionis', 'transparent', 'tre', 'trevor', 'tribal', 'trol', 'trouble', 'true', 'trumps', 'trust', 'ts', 'tt', 'ttps3a', 'tubb', 'tuberculosis', 'tudy', 'tue', 'tuesday', 'tuler', 'turmeric', 'tute', 'twenty', 'twitter', 'two', 'twostep', 'ty', 'typ', 'types', 'uae', 'ual', 'uch', 'uci', 'ucsf', 'ugh', 'ught', 'uld', 'ully', 'uman', 'unce', 'und', 'unde', 'undergo', 'underlying', 'understand', 'understands', 'underway', 'uni', 'unive', 'universityas', 'universityin', 'unknown', 'unknowns', 'unpublished', 'unsubscribe', 'unt', 'uoxouwu', 'uper', 'ur', 'ure', 'ures', 'urldefense', 'urse', 'usage', 'useful', 'using', 'ust', 'ute', 'utes', 'utm', 'v78hfi7t', 'vacc', 'vacci', 'vaccines', 'vaccinologists', 'valuable', 'value', 'vancouverthe', 'varadara', 'variability', 'ved', 'vel', 'vely', 'ven', 'vendors', 'ver', 'vermont', 'vero', 'version', 'versions', 'versusevacuatingthem', 'ves', 'veteran', 'vice', 'victoria', 'vid', 'vid19', 'vide', 'videol', 'viewpoints', 'vir', 'viraloq', 'viru', 'virusdeathtoll', 'visby', 'vmm4xrunhf', 'vo419b2jrhebfjmsqcd6byktehqq', 'voices', 'vp', 'vpdate', 'vulnerable', 'wal', 'walm', 'walma', 'walmaiis', 'wan', 'wanta', 'wanted', 'warmer', 'warren', 'washington', 'washingtonpost', 'watch', 'wear', 'webb', 'webby', 'webcasts', 'webex', 'webs', 'wednesday', 'wednesdays', 'weigh', 'weill', 'wel', 'welj', 'well', 'welltrained', 'weve', 'wh', 'wha', 'whenever', 'whi', 'whic', 'whit', 'white', 'whlle', 'wholeheartedly', 'wi', 'widely', 'widespread', 'william', 'willing', 'wilson', 'wind', 'windo', 'window', 'wish', 'wo', 'wolf', 'wolfe', 'wond', 'workers', 'workforce', 'working', 'worldwide', 'worried', 'worse', 'worst', 'wou', 'woul', 'wpierceapcoworldwide', 'writing', 'wro', 'wrong', 'wrot', 'ws', 'www', 'x', 'yearly', 'years', 'yes', 'yh', 'yikes', 'youd', 'young', 'youngspgloba', 'youre', 'ypes', 'ysis', 'yz', 'z', 'zac', 'zaks', 'zdk', 'zee', 'zeke', 'zhu', 'zika', 'zpevda', 'zuck'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nagak\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:497: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Re initiating the model by removing the inappropriate key words as stopwords and ngram selection\n",
    "\n",
    "'''\n",
    "\n",
    "train_features, test_features, train_word_features, train_char_features, y_train, y_test, test_text, word_vectorizer, char_vectorizer = preprocesse_text(df, test_size=0.2, stopwords=nltk.corpus.stopwords.words('english')+extra_stopwords)\n",
    "\n",
    "reg = LinearRegression()\n",
    "cv_score = np.mean(cross_val_score(reg, train_features, y_train.values, cv=3, scoring='r2'))\n",
    "reg.fit(train_features, y_train)\n",
    "\n",
    "te = pd.DataFrame()\n",
    "te['text'] = test_text\n",
    "te['preds'] = reg.predict(test_features)\n",
    "te['target'] = y_test\n",
    "te['diff'] = te['target']-te['preds']\n",
    "te['percent'] = te['diff'].abs().div(te['target'])\n",
    "te = te.sort_values('percent')\n",
    "\n",
    "features = pd.DataFrame()\n",
    "features['weights'] = pd.Series(reg.coef_)\n",
    "features['features'] = word_vectorizer.get_feature_names()+char_vectorizer.get_feature_names()\n",
    "features['wgt'] = features['weights'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQzklEQVR4nO3dbYxcZ3nG8f/GG9sybNwFBlKkqG4FupsiQYopCQm2VyGQmpcGRRUgFBCEpmll3qPm1SlqFQSpwECAQNlgudAiKhKsQlo3qIS6TlqUFiUSVs2dJoCQKkDbsDabmhhsbz/MWTqZrHdnz56d2Xny/32aOeeZM5ePJ9c8+/iczcjs7CySpDKcNugAkqTmWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQUZ7WVQRJwL3JyZEx3b3gi8IzNfUj2/ArgSOA7clJl3LnbcqamZ2tdTjo9vYHr6aN2X9405mzUMOYchI5izaf3M2WqNjZxq36Iz9Yi4GrgNWN+x7RzgbcBI9fxM4J3ABcDFwAciYt2yUi9idHTNSh6+MeZs1jDkHIaMYM6mrZacvSy/PAxcOvckIp4OfBB4d8eYFwP3ZuaxzDwCPAQ8v8GckqQeLLr8kpl3RMQmgIhYA3wWeA/ws45hZwBHOp7PABsXO/b4+IZlfbu1WmO1X9tP5mzWMOQchoxgzqathpw9ral32Aw8F/gU7eWY34qIjwJ3A51/mjHg8GIHW876U6s1xtTUTO3X94s5mzUMOYchI5izaf3MudCXx5JKPTPvA54HUM3ev5iZ767W1N8fEeuBdcDZwMG6gSVJ9TRySWNm/gi4BThAe9Z+Q2Y+1sSxJUm962mmnpnfB85baFtmTgKTDWaTJC2RNx9JUkEsdUkqiKUuSQVZ6iWNq8Zrrvq7gb337msvHNh7S9JCnKlLUkEsdUkqiKUuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUEEtdkgpiqUtSQSx1SSqIpS5JBenp/1EaEecCN2fmREScA3wcOAEcA96cmT+OiCuAK4HjwE2ZeecKZZYkncKiM/WIuBq4DVhfbfoY8I7MnAC+DFwTEWcC7wQuAC4GPhAR61YksSTplHpZfnkYuLTj+Rsy84Hq8SjwGPBi4N7MPJaZR4CHgOc3GVSStLhFl18y846I2NTx/IcAEXE+8HZgK+3Z+ZGOl80AGxc79vj4BkZH1ywx8uC1WmMrOn5QzNmcYcgI5mzaasjZ05p6t4h4PXAD8KrMnIqInwKdf5ox4PBix5mePlrn7Qduamqm57Gt1tiSxg+KOZszDBnBnE3rZ86FvjyWXOoRcRntfxCdyMyfVJvvA94fEeuBdcDZwMGlR5UkLceSSj0i1gC3AD8AvhwRAPsz830RcQtwgPY6/Q2Z+VjTYSVJC+up1DPz+8B51dOnnWLMJDDZTCxJUh3efCRJBbHUJakglrokFcRSl6SCWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUEEtdkgpiqUtSQUZ7GRQR5wI3Z+ZERDwH2APMAgeBHZl5MiKuAK4EjgM3ZeadK5RZknQKi87UI+Jq4DZgfbVpF7AzM7cAI8AlEXEm8E7gAuBi4AMRsW5lIkuSTqWXmfrDwKXA56vnm4H91eN9wCuAE8C9mXkMOBYRDwHPB/59oQOPj29gdHRNndwD1WqNrej4QTFnc4YhI5izaash56Klnpl3RMSmjk0jmTlbPZ4BNgJnAEc6xsxtX9D09NHek64iU1MzPY9ttcaWNH5QzNmcYcgI5mxaP3Mu9OVR5x9KT3Y8HgMOAz+tHndvlyT1UZ1Svz8iJqrH24EDwH3AlohYHxEbgbNp/yOqJKmPerr6pctVwGRErAUOAbdn5omIuIV2wZ8G3JCZjzWYU5LUg55KPTO/D5xXPX4Q2DbPmElgsslwkqSl8eYjSSqIpS5JBbHUJakglrokFcRSl6SCWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUkNE6L4qI04G/AjYBJ4ArgOPAHmAWOAjsyMyTjaSUJPWk7kz9lcBoZp4P/DnwfmAXsDMztwAjwCXNRJQk9apuqT8IjEbEacAZwC+AzcD+av8+4KLlx5MkLUWt5RfgUdpLL98BngG8GtiambPV/hlg42IHGR/fwOjompoRBqfVGlvR8YNizuYMQ0YwZ9NWQ866pf4e4K7MvC4izgLuBtZ27B8DDi92kOnpozXffrCmpmZ6HttqjS1p/KCYsznDkBHM2bR+5lzoy6Pu8ss0cKR6/BPgdOD+iJiotm0HDtQ8tiSpproz9Y8AuyPiAO0Z+vXAfwCTEbEWOATc3kxESVKvapV6Zj4KvG6eXduWF0eStBzefCRJBbHUJakglrokFcRSl6SCWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUEEtdkgpiqUtSQUbrvjAirgN+D1gL3ArsB/YAs8BBYEdmnmwgoySpR7Vm6hExAZwPXABsA84CdgE7M3MLMAJc0lBGSVKP6i6/XAx8G9gLfBW4E9hMe7YOsA+4aNnpJElLUnf55RnArwGvBn4d+ApwWmbOVvtngI2LHWR8fAOjo2tqRhicVmtsRccPijmbMwwZwZxNWw0565b6I8B3MvPnQEbEY7SXYOaMAYcXO8j09NGabz9YU1MzPY9ttcaWNH5QzNmcYcgI5mxaP3Mu9OVRd/nlHuB3I2IkIp4NPAX4erXWDrAdOFDz2JKkmmrN1DPzzojYCtxH+4thB/A9YDIi1gKHgNsbSylJ6kntSxoz8+p5Nm9bRhZJ0jJ585EkFcRSl6SCWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUEEtdkgpiqUtSQSx1SSqIpS5JBRldzosj4pnAt4CXA8eBPcAscBDYkZknlxtQktS72jP1iDgd+EvgZ9WmXcDOzNwCjACXLD+eJGkpljNT/xDwaeC66vlmYH/1eB/wCmDvQgcYH9/A6OiaZUQYjFZrbEXHD4o5mzMMGcGcTVsNOWuVekS8BZjKzLsiYq7URzJztno8A2xc7DjT00frvP3ATU3N9Dy21Rpb0vhBMWdzhiEjmLNp/cy50JdH3Zn65cBsRFwEnAN8Dnhmx/4x4HDNY0uSaqq1pp6ZWzNzW2ZOAA8Abwb2RcRENWQ7cKCJgJKk3i3r6pcuVwGTEbEWOATc3uCxJUk9WHapV7P1OduWezxJUn3efCRJBbHUJakglrokFcRSl6SCWOqSVBBLXZIKYqlLUkEsdUkqiKUuSQWx1CWpIE3+7pcnjcs/ePdA3nf3tRcO5H0lDQ9n6pJUEEtdkgpiqUtSQSx1SSqIpS5JBbHUJakglrokFcRSl6SCWOqSVBBLXZIKYqlLUkFq/e6XiDgd2A1sAtYBNwH/CewBZoGDwI7MPNlISklST+rO1C8DHsnMLcB24BPALmBntW0EuKSZiJKkXtUt9S8BN3Y8Pw5sBvZXz/cBFy0jlySphlrLL5n5KEBEjAG3AzuBD2XmbDVkBti42HHGxzcwOrqmToQnpVZrbKiP35RhyDkMGcGcTVsNOWv/PvWIOAvYC9yamV+IiL/o2D0GHF7sGNPTR+u+/ZPS1NTMih271Rpb0eM3ZRhyDkNGMGfT+plzoS+PWssvEfEs4GvANZm5u9p8f0RMVI+3AwfqHFuSVF/dmfr1wDhwY0TMra2/C7glItYCh2gvy0iS+qjumvq7aJd4t23LiyNJWg5vPpKkgljqklQQS12SCmKpS1JBLHVJKoilLkkFqX1Hqfrv8g/ePbD33n3thQN7b0m9c6YuSQWx1CWpIJa6JBXEUpekgljqklQQS12SCmKpS1JBvE5dPRnkNfKD4HX5Tw4l3vvhTF2SCmKpS1JBLHVJKoilLkkFsdQlqSCWuiQVxEsapXmUeKnbavZku2R2JTlTl6SCNDpTj4jTgFuBFwDHgD/IzIeafA9JK8PZchmanqm/FlifmS8BrgU+3PDxJUkLaLrUXwr8I0BmfhN4UcPHlyQtYGR2draxg0XEbcAdmbmvev4D4Dcy83hjbyJJOqWmZ+o/BcY6j2+hS1L/NF3q9wKvBIiI84BvN3x8SdICmr5OfS/w8oj4V2AEeGvDx5ckLaDRNXVJ0mB585EkFcRSl6SCrPrf/bLYXaoR8RrgT4HjwO7MnBxAxtOB3cAmYB1wU2Z+pWP/e4G3AVPVpiszM/uds8pyP3Ckevq9zHxrx76Bn8sqx1uAt1RP1wPnAGdm5uFq/8DPZ0ScC9ycmRMR8RxgDzALHAR2ZObJjrEDudO6K+M5wMeBE1WGN2fmj7vGn/Kz0cecLwS+CvxXtftTmfm3HWMHdtd6V84vAmdWuzYB38zMN3SNH8j5XPWlTsddqtUVNR8GLoFflulHgN8B/he4NyK+mpk/6nPGy4BHMvNNEfF04H7gKx37X0j7P6Jv9TnX40TEeoDMnJhn32o5l2TmHtolSUR8kvYXzOGOIQM9nxFxNfAm2ucJYBewMzP/OSI+TfvzubfjJa/lFJ/hPmb8GPCOzHwgIq4ErgHe2zH+lJ+NPud8IbArM091N/pr6fO5nC/nXIFHxDjwDeA9XeMHcj5hOJZfFrpL9WzgocyczsyfA/cAW/ofkS8BN3Y87742fzNwXUTcExHX9S/WE7wA2BARX4uIu6v/KOaslnP5SxHxIuB5mfmZrl2DPp8PA5d25dlfPd4HXNQ1fhB3WndnfENmPlA9HgUe6xq/0GdjJc13Ll8VEf8SEZ+NiLGu8YO6a70755w/Az6emT/s2j6o8zkUpX4G//8jDMCJiBg9xb4ZYGO/gs3JzEczc6b6AN4O7Owa8kXgj4ALgZdGxKv7nbFyFPgQcHGV529W27nscj3t/2i6DfR8ZuYdwC86No1k5txlZPOdt4U+wyuiO+Nc6UTE+cDbaf9U1mmhz0bfcgL3AX+SmVuB7wLv63pJ388lzJuTiHgm8DKqnyq7DOR8wnCU+kJ3qXbvGwMO9ynX40TEWbR/DPt8Zn6hY/sI8NHM/J9qBvz3wG8PIiPwIPDXmTmbmQ8CjwC/Wu1bNecSICJ+BfjNzPxG1/bVdD7nnOx4PN95WxV3WkfE64FPA6/KzKmu3Qt9Nvppb8ey2l6e+He7Ks5l5feBL2TmiXn2Dex8DkOpL3SX6iHguRHxtIhYC2wF/q3fASPiWcDXgGsyc3fX7jOAgxHx1KqQLgQGtbZ+OdVvzoyIZ1fZ5n5sXBXnssNW4J/m2b6azuec+yNionq8HTjQtX/gd1pHxGW0Z+gTmfndeYYs9Nnop7si4sXV45fxxL/bgZ/LDhfRXm6bz8DO5zD8Q+kT7lKNiDcCT83Mz1RXQtxF+wtqd2b+9wAyXg+MAzdGxNza+iTwlCrj9bRn8ceAr2fmPwwgI8BngT0RcQ/tKzUuB14XEavpXM4J2j9+t588/u98tZzPOVcBk9WX4SHaS3BExOdoL8UN9E7riFgD3AL8APhyRADsz8z3dWR8wmdjQDPgPwY+ERE/B34E/GH1Z1gV57LL4z6j8LicAzuf3lEqSQUZhuUXSVKPLHVJKoilLkkFsdQlqSCWuiQVxFKXpIJY6pJUEEtdkgryfzWvI+93ew8SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.iloc[:int(len(te)*0.9)]['percent'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the generated model\n",
    "with open('regressor.pkl','wb') as f:\n",
    "    pickle.dump(reg, f)\n",
    "    pickle.dump(word_vectorizer, f)\n",
    "    pickle.dump(char_vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
